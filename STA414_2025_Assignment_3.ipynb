{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/javmencia/COBWEBfiles/blob/main/STA414_2025_Assignment_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Changelog:  (Last Updated 2025-02-26)"
      ],
      "metadata": {
        "id": "gMAXaY8USfWb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Probabilistic ML: Assignment 3\n",
        "- **Deadline**: 2025-03-16 (March 16th 2025, 23:59)\n",
        "- **Submission**: You need to submit your solutions through Crowdmark, including all your derivations, plots, and your code. You can produce the files however you like (e.g. $\\LaTeX$, Microsoft Word, etc), as long as it is readable. Points will be deducted if we have a hard time reading your solutions or understanding the structure of your code.\n",
        "\n",
        "- **Collaboration policy**: After attempting the problems on an individual basis, you may discuss and work together on the assignment with up to two classmates. However, **you must write your own code and write up your own solutions individually and explicitly name any collaborators** at the top of the homework."
      ],
      "metadata": {
        "id": "248_qtulv8jx"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t1RKToQQcX2Q"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        "# This is formatted as code\n",
        "```\n",
        "\n",
        "# 1. [54pts] Stochastic Variational Inference in the TrueSkill Model\n",
        "\n",
        "## Background\n",
        "\n",
        "We'll continue working with [TrueSkill](http://papers.nips.cc/paper/3079-trueskilltm-a-bayesian-skill-rating-system.pdf) model, a player ranking system for competitive games originally developed for Halo 2. Recall the model:\n",
        "\n",
        "\n",
        "## Model definition\n",
        "\n",
        "We assume that each player has a true, but unknown skill $z_i \\in \\mathbb{R}$.\n",
        "We use $N$ to denote the number of players.\n",
        "\n",
        "### The prior:\n",
        "The prior over each player's skill is a standard normal distribution, and all player's skills are *a priori* independent.\n",
        "\n",
        "### The likelihood:\n",
        "For each observed game, the probability that player $i$ beats player $j$, given the player's skills $z_A$ and $z_B$, is:\n",
        "$$p(A \\,\\, \\text{beat} \\,\\, B | z_A, z_B) = \\sigma(z_i - z_j)$$\n",
        "where\n",
        "$$\\sigma(y) = \\frac{1}{1 + \\exp(-y)}$$\n",
        "We chose this function simply because it's close to zero or one when the player's skills are very different, and equals one-half when the player skills are the same.  This likelihood function is the only thing that gives meaning to the latent skill variables $z_1 \\dots z_N$.\n",
        "\n",
        "There can be more than one game played between a pair of players. The outcome of each game is independent given the players' skills.\n",
        "We use $M$ to denote the number of games.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n6euHm_h4Voi"
      },
      "outputs": [],
      "source": [
        "!pip install wget\n",
        "import os\n",
        "import os.path\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import wget\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "from scipy.stats import norm\n",
        "import scipy.io\n",
        "import scipy.stats\n",
        "import torch\n",
        "import random\n",
        "from torch import nn\n",
        "from torch.distributions.normal import Normal\n",
        "\n",
        "from functools import partial\n",
        "from tqdm import trange, tqdm_notebook\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Helper function\n",
        "def diag_gaussian_log_density(x, mu, std):\n",
        "    # axis=-1 means sum over the last dimension.\n",
        "    m = Normal(mu, std)\n",
        "    return torch.sum(m.log_prob(x), axis=-1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-bf1iLXTkXd5"
      },
      "source": [
        "## Implementing the TrueSkill Model\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A62Y2IWg39ix"
      },
      "source": [
        "This part was mostly done in Assignment 2. We will recall some useful functions.\n",
        "\n",
        "**a)** The function $\\texttt{log_joint_prior}$ computes the log of the prior, jointly evaluated over all player's skills."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PDmHDiac4Rpr"
      },
      "outputs": [],
      "source": [
        "def log_joint_prior(zs_array):\n",
        "    return diag_gaussian_log_density(zs_array, torch.tensor([0.0]), torch.tensor([1.0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AAbAgxMl4X2q"
      },
      "source": [
        "**b)** The function `logp_a_beats_b` that, given a pair of skills $z_a$ and $z_b$, evaluates the log-likelihood that player with skill $z_a$ beat player with skill $z_b$ under the model detailed above.\n",
        "\n",
        "To ensure numerical stability, we use the function `np.log1p` that computes $\\log(1 + x)$ in a numerically stable way.  Or even better, use `np.logaddexp`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V7nUXkYl_f4T"
      },
      "outputs": [],
      "source": [
        "def logp_a_beats_b(z_a, z_b):\n",
        "    return -torch.logaddexp(torch.tensor([0.0]), z_b - z_a)\n",
        "\n",
        "def log_prior_over_2_players(z1, z2):\n",
        "    m = Normal(torch.tensor([0.0]), torch.tensor([[1.0]]))\n",
        "    return m.log_prob(z1) + m.log_prob(z2)\n",
        "\n",
        "def prior_over_2_players(z1, z2):\n",
        "    return torch.exp(log_prior_over_2_players(z1, z2))\n",
        "\n",
        "def log_posterior_A_beat_B(z1, z2):\n",
        "    return log_prior_over_2_players(z1, z2) + logp_a_beats_b(z1, z2)\n",
        "\n",
        "def posterior_A_beat_B(z1, z2):\n",
        "    return torch.exp(log_posterior_A_beat_B(z1, z2))\n",
        "\n",
        "def log_posterior_A_beat_B_20_times(z1, z2):\n",
        "    return log_prior_over_2_players(z1, z2) + 20.0 * logp_a_beats_b(z1, z2)\n",
        "\n",
        "def posterior_A_beat_B_20_times(z1, z2):\n",
        "    return torch.exp(log_posterior_A_beat_B_20_times(z1, z2))\n",
        "\n",
        "def log_posterior_beat_each_other_20_times(z1, z2):\n",
        "    return log_prior_over_2_players(z1, z2) \\\n",
        "        + 20.* logp_a_beats_b(z1, z2) \\\n",
        "        + 20.* logp_a_beats_b(z2, z1)\n",
        "\n",
        "def posterior_beat_each_other_20_times(z1, z2):\n",
        "    return torch.exp(log_posterior_beat_each_other_20_times(z1, z2))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following functions will be used for plotting.  Note that `plot_2d_fun` can now take an optional second function, so you can compare two functions."
      ],
      "metadata": {
        "id": "hA77ZVR5SgGs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "at7JEyWs_QRK"
      },
      "outputs": [],
      "source": [
        "# Plotting helper functions for free\n",
        "def plot_isocontours(ax, func, xlimits=[-4, 4], ylimits=[-4, 4], steps=101, cmap=\"summer\"):\n",
        "    x = torch.linspace(*xlimits, steps=steps)\n",
        "    y = torch.linspace(*ylimits, steps=steps)\n",
        "    X, Y = torch.meshgrid(x, y)\n",
        "    Z = func(X, Y)\n",
        "    plt.contour(X, Y, Z, cmap=cmap)\n",
        "    ax.set_yticks([])\n",
        "    ax.set_xticks([])\n",
        "\n",
        "def plot_2d_fun(f, x_axis_label=\"\", y_axis_label=\"\", f2=None, scatter_pts=None):\n",
        "    # This is the function your code should call.\n",
        "    # f() should take two arguments.\n",
        "    fig = plt.figure(figsize=(8,8), facecolor='white')\n",
        "    ax = fig.add_subplot(111, frameon=False)\n",
        "    ax.set_xlabel(x_axis_label)\n",
        "    ax.set_ylabel(y_axis_label)\n",
        "    plot_isocontours(ax, f)\n",
        "    if f2 is not None:\n",
        "      plot_isocontours(ax, f2, cmap='winter')\n",
        "\n",
        "    if scatter_pts is not None:\n",
        "      plt.scatter(scatter_pts[:,0], scatter_pts[:, 1])\n",
        "    plt.plot([4, -4], [4, -4], 'b--')   # Line of equal skill\n",
        "    plt.show(block=True)\n",
        "    plt.draw()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nm-SM6Fc4yz8"
      },
      "source": [
        "## **1.1 [20pts]** Stochastic Variational Inference on Two Players and Toy Data\n",
        "\n",
        "One nice thing about a Bayesian approach is that it separates the model specification from the approximate inference strategy.\n",
        "The original Trueskill paper from 2007 used message passing.\n",
        "\n",
        "In this question we will  approximate posterior distributions with gradient-based stochastic variational inference.\n",
        "\n",
        "The parameters are $\\phi = (\\mu,\\log(\\sigma))$. Notice that instead of $\\sigma$ (which is constrained to be positive), we work with $\\log(\\sigma)$, removing the constraint. This way, we can do unconstrained gradient-based optimization.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K-ri8HRQ461m"
      },
      "source": [
        "**a) [9pts]** Implement the missing lines in the below code, to complete the evidence lower bound function and the reparameterized sampler for the approximate posterior.\n",
        "\n",
        "Hint 1: You must use the reparametrization trick in your sampler if you want your gradients to be unbiased.\n",
        "\n",
        "Hint 2: If you're worried you got these wrong, you can check that the sampler matches the log pdf by plotting a histogram of samples against a plot of the pdf."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kZlCOViq5Ahf"
      },
      "outputs": [],
      "source": [
        "def diag_gaussian_samples(mean, log_std, num_samples):\n",
        "    # mean and log_std are (D) dimensional vectors\n",
        "    # Return a (num_samples, D) matrix, where each sample is\n",
        "    # from a diagonal multivariate Gaussian.\n",
        "\n",
        "    # TODO.  You might want to use torch.randn(). Remember\n",
        "    # you must use the reparameterization trick.  Also remember that\n",
        "    # we are parameterizing the _log_ of the standard deviation.\n",
        "\n",
        "\n",
        "def diag_gaussian_logpdf(x, mean, log_std):\n",
        "    # Evaluate the density of a batch of points on a\n",
        "    # diagonal multivariate Gaussian. x is a (num_samples, D) matrix.\n",
        "    # Return a tensor of shape (num_samples)\n",
        "\n",
        "def batch_elbo(logprob, mean, log_std, num_samples):\n",
        "    # TODO: Use simple Monte Carlo to estimate ELBO\n",
        "    # on a batch of size num_samples\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lfAD3ffb5CD9"
      },
      "source": [
        "\n",
        "**b) [4pts]** Write a loss function called $\\texttt{objective}$  that takes variational distribution parameters, and returns an unbiased estimate of the negative elbo using $\\texttt{num_samples_per_iter}$ samples, to approximate the joint posterior over skills conditioned on observing player A winning 20 games.\n",
        "\n",
        "Note: We want a _negative_ ELBO estimate, because the convention in optimization is to minimize functions, and we want to maximize the ELBO."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters\n",
        "num_players = 2\n",
        "n_iters = 1200\n",
        "stepsize = 0.0001\n",
        "num_samples_per_iter = 80\n",
        "\n",
        "def log_posterior_A_beat_B_20_times_1_arg(z1z2):\n",
        "  return log_posterior_A_beat_B_20_times(z1z2[:,0], z1z2[:,1]).flatten()\n",
        "\n",
        "def objective(params):  # The loss function to be minimized.\n",
        "  # TODO.  Hint:  This can be done in one line.\n"
      ],
      "metadata": {
        "id": "j-Qt2vqvKgAN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**c) [1pts]** Initialize a set of variational parameters and optimize them to approximate the joint where we observe player A winning 20 games. Report the final loss. Also plot the optimized variational approximation contours and the target distribution on the same axes.\n",
        "\n",
        "Hint:  Any initialization should be fine.  How many variational parameters do you need?"
      ],
      "metadata": {
        "id": "ixGDUDjwKnWd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "biW5JiZq5EXc"
      },
      "outputs": [],
      "source": [
        "def callback(params, t):\n",
        "  if t % 25 == 0:\n",
        "    print(\"Iteration {} lower bound {}\".format(t, objective(params)))\n",
        "\n",
        "# Set up optimizer.\n",
        "D = 2\n",
        "# init_log_std  = # TODO.\n",
        "# init_mean = # TODO\n",
        "\n",
        "\n",
        "params = (init_mean, init_log_std)\n",
        "optimizer = torch.optim.SGD(params, lr=stepsize, momentum=0.9)\n",
        "\n",
        "def update():\n",
        "    optimizer.zero_grad()\n",
        "    loss = objective(params)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "# Main loop.\n",
        "print(\"Optimizing variational parameters...\")\n",
        "for t in trange(0, n_iters):\n",
        "    update()\n",
        "    callback(params, t)\n",
        "\n",
        "\n",
        "def approx_posterior_2d(z1, z2):\n",
        "    # The approximate posterior\n",
        "    mean, logstd = params[0].detach(), params[1].detach()\n",
        "    return torch.exp(diag_gaussian_logpdf(torch.stack([z1, z2], dim=2), mean, logstd))\n",
        "\n",
        "plot_2d_fun(posterior_A_beat_B_20_times, \"Player A Skill\", \"Player B Skill\",\n",
        "            f2=approx_posterior_2d)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sCmqGbeK5iAf"
      },
      "source": [
        "**d) [3pt]** Write a loss function called $\\texttt{objective}$  that takes variational distribution parameters , and returns a negative elbo estimate using simple Monte carlo with $\\texttt{num_samples_per_iter}$ samples, to approximate the joint where we observe player A winning 10 games and player B winning 20 games.\n",
        "\n",
        "Hint:  You can find analogous functions in the code above.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters\n",
        "n_iters = 100\n",
        "stepsize = 0.0001\n",
        "num_samples_per_iter = 50\n",
        "\n",
        "def log_posterior_beat_each_other_20_times_1_arg(z1z2):\n",
        "    # z1z2 is a tensor with shape (num_samples x 2)\n",
        "    # Return a tensor with shape (num_samples)\n",
        "\n",
        "\n",
        "def objective(params):\n"
      ],
      "metadata": {
        "id": "5Zz6DkOacXDC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**e) [3pt]** Run the code below to optimize, and report the final loss. Also plot the optimized variational approximation contours and the target distribution on the same axes.\n",
        "\n",
        "Write one or two sentences describing the joint settings of skills that are plausible under the true posterior, but which are not plausible under the approximate posterior.\n",
        "\n",
        "Finally, answer with one or two sentences:  Would changing the variational approximate posterior from a fully-factorized (diagonal covariance) Gaussian to a non-factorized (fully parameterized covariance) Gaussian make a better approximation in this instance?"
      ],
      "metadata": {
        "id": "_V42MMIscvMs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ksjggALU5gWx"
      },
      "outputs": [],
      "source": [
        "# Main loop.\n",
        "print(\"Optimizing variational parameters...\")\n",
        "for t in trange(0, n_iters):\n",
        "    update()\n",
        "    callback(params, t)\n",
        "\n",
        "plot_2d_fun(posterior_beat_each_other_20_times, \"Player A Skill\", \"Player B Skill\",\n",
        "            f2=approx_posterior_2d)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rrCmtd_b51Jf"
      },
      "source": [
        "## 1.2 [34 pts] Approximate inference conditioned on real data\n",
        "\n",
        "The dataset contains data on 2500 games amongst 33 Premier League teams:\n",
        " - names is a 33 by 1 matrix, whose $i$’th entry is the name of player $i$.\n",
        " - games is a 2500 by 2 matrix of game outcomes, one row per game.\n",
        "\n",
        "The first column contains the indices of the team who won.\n",
        "The second column contains the indices of the team who lost.\n",
        "\n",
        "It is based on the following kaggle dataset: https://www.kaggle.com/datasets/evangower/premier-league-matches-19922022\n",
        "\n",
        "<!-- The dataset contains data on 2546 chess games amongst 1434 players:\n",
        " - names is a 1434 by 1 matrix, whose $i$’th entry is the name of player $i$.\n",
        " - games is a 2546 by 2 matrix of game outcomes (actually chess matches), one row per game.\n",
        "\n",
        "The first column contains the indices of the players who won.\n",
        "The second column contains the indices of the player who lost.\n",
        "\n",
        "It is based on the kaggle chess dataset: https://www.kaggle.com/datasets/datasnaek/chess -->\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the dataset\n",
        "!curl -L -o premier-league-matches-19922022.zip\\\n",
        "  https://www.kaggle.com/api/v1/datasets/download/evangower/premier-league-matches-19922022\n",
        "!unzip premier-league-matches-19922022.zip"
      ],
      "metadata": {
        "id": "n6mmQQMVdCNv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "def load_games():\n",
        "    dataset = pd.read_csv(\"premier-league-matches.csv\")\n",
        "    mini_ds = dataset[dataset['FTR'] != 'D'][-2500:]\n",
        "    all_teams = pd.concat((mini_ds['Home'], mini_ds['Away'])).unique()\n",
        "    encoder = LabelEncoder()\n",
        "    encoder.fit(all_teams)\n",
        "    mini_ds['HomeId'] = encoder.transform(mini_ds['Home'])\n",
        "    mini_ds['AwayId'] = encoder.transform(mini_ds['Away'])\n",
        "\n",
        "    winner_ids = np.where(mini_ds['FTR'] == 'H', mini_ds['HomeId'], mini_ds['AwayId'])\n",
        "    loser_ids = np.where(mini_ds['FTR'] == 'H', mini_ds['AwayId'], mini_ds['HomeId'])\n",
        "    games = np.column_stack((winner_ids, loser_ids))\n",
        "    names = encoder.classes_\n",
        "\n",
        "    return games, names\n",
        "\n",
        "games, names = load_games()\n",
        "games = torch.LongTensor(games)"
      ],
      "metadata": {
        "id": "G65iLniidCUn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dkYB8hS07fOW"
      },
      "source": [
        "\n",
        "\n",
        "**a) [0pt]** Assuming all game outcomes are i.i.d. conditioned on all players' skills, the function $\\texttt{log_games_likelihood}$ takes a batch of player skills $\\texttt{zs}$ and a collection of observed games $\\texttt{games}$ and gives the total log-likelihood for all those observations given all the skills.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pkW93RBf7pru"
      },
      "outputs": [],
      "source": [
        "def log_games_likelihood(zs, games):\n",
        "    winning_player_ixs = games[:,0]\n",
        "    losing_player_ixs = games[:,1]\n",
        "\n",
        "    winning_player_skills = zs[:, winning_player_ixs]\n",
        "    losing_player_skills = zs[:, losing_player_ixs]\n",
        "\n",
        "    log_likelihoods = logp_a_beats_b(winning_player_skills, losing_player_skills)\n",
        "    return torch.sum(log_likelihoods, dim=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uaPEulQoAndL"
      },
      "outputs": [],
      "source": [
        "def log_joint_probability(zs):\n",
        "    return log_joint_prior(zs) + log_games_likelihood(zs, games)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aUWILTA18BRn"
      },
      "source": [
        "**b) [4pt]** Write a new objective function like the one from the previous question.\n",
        "\n",
        "Below, we initialize a variational distribution and fit it to the joint distribution with all the observed tennis games from the dataset."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters\n",
        "num_players = 33\n",
        "n_iters = 200\n",
        "stepsize = 0.0001\n",
        "num_samples_per_iter = 50\n",
        "\n",
        "def objective(params):\n"
      ],
      "metadata": {
        "id": "qsCQUJCFd2Eb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**c) [3pts]** Optimize, and report the final loss."
      ],
      "metadata": {
        "id": "O3ep43C5d_CT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ock35XuW8EK8"
      },
      "outputs": [],
      "source": [
        "# Set up optimizer.\n",
        "init_mean = torch.zeros(num_players, requires_grad=True)\n",
        "init_log_std  = torch.zeros(num_players, requires_grad=True)\n",
        "params = (init_mean, init_log_std)\n",
        "optimizer = torch.optim.SGD(params, lr=stepsize, momentum=0.9)\n",
        "\n",
        "def update():\n",
        "    optimizer.zero_grad()\n",
        "    loss = objective(params)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "# Optimize and print loss in a loop\n",
        "# HINT: you can use the callback() function to report loss\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y6YluZ2B8Qmr"
      },
      "source": [
        "**d) [1pt]** Plot the approximate mean and variance of all players, sorted by skill."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sEZtYBeH8TOO"
      },
      "outputs": [],
      "source": [
        "# mean_skills, logstd_skills = # TODO.  Hint: You don't need to do simple Monte Carlo here.\n",
        "# Hint: You should use .detach() before you do anything with the params tensors\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EjWom7_Z8VCY"
      },
      "source": [
        "**e) [1pts]** List the names of the 10 players with the highest mean skill under the variational model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4LCPM5Bp8YiK"
      },
      "outputs": [],
      "source": [
        "for i in range(1,11):\n",
        "    # TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sEj6LkYA8ncH"
      },
      "source": [
        "**f) [3pt]** Plot samples from the joint posterior over the skills of Arsenal and Liverpool. Based on your samples, describe in a sentence the relationship between the skills of the teams. (Is one better than the other? Are they approximately even?)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U6zI2sCB8qFO"
      },
      "outputs": [],
      "source": [
        "arsenal_ix = 0\n",
        "liverpool_ix = 15\n",
        "print(names[arsenal_ix])\n",
        "print(names[liverpool_ix])\n",
        "\n",
        "fig = plt.figure(figsize=(8,8), facecolor='white')\n",
        "\n",
        "# Label each with \"<player> Skill\"\n",
        "plt.xlabel(\"Arsenal Skill\")\n",
        "plt.ylabel(\"Liverpool Skill\")\n",
        "\n",
        "plt.plot([3, -3], [3, -3], 'b--') # Line of equal skill\n",
        "\n",
        "samples = diag_gaussian_samples(mean_skills, logstd_skills, 100)\n",
        "\n",
        "# TODO:  Hint:  Use plt.scatter()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rUXcsAYdDnor"
      },
      "source": [
        "**g) [8pts]** Derive the exact probability under a factorized Gaussian over two players’ skills that one has higher skill than the other, as a function of the two means and variances over their skills. Express your answer in terms of the cumulative distribution function of a one-dimensional Gaussian random variable.\n",
        "\n",
        "- Hint 1: Use a linear change of variables $y_A, y_B = z_A − z_B , z_B$. What does the line of equal skill look like after this transformation?\n",
        "- Hint 2: If $X \\sim \\mathcal N (\\mu, \\Sigma)$, then $AX \\sim \\mathcal N (A\\mu, A\\Sigma A^T)$ where $A$ is a linear transformation.\n",
        "- Hint 3: Marginalization in Gaussians is easy: if $X \\sim \\mathcal N (\\mu, \\Sigma)$, then the $i$th element of $X$ has a\n",
        "marginal distribution $X_i \\sim \\mathcal N (\\mu_i , \\Sigma_{ii})$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OxazFdqcFhF_"
      },
      "source": [
        "Your answer here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GO3k7rfI8sWg"
      },
      "source": [
        "**h) [4pts]** Compute the probability under your approximate posterior that Arsenal has higher skill than Liverpool. Compute this quantity exactly using the formula you just derived above, and also estimate it using simple Monte Carlo with 10000 examples.\n",
        "\n",
        "Hint:  You might want to use `Normal(0,1).cdf()` for the exact formula."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oqXgDi-T-W7o"
      },
      "outputs": [],
      "source": [
        "# TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "olpPTnm3-YdN"
      },
      "source": [
        "**i) [2pts]** Compute the probability that Arsenal is better than the player with the 5th lowest mean skill. Compute this quantity exactly, and then estimate it using simple Monte Carlo with 10000 examples.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO\n"
      ],
      "metadata": {
        "id": "PNLfQCDdbZ7d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**j) [4 pts]** Imagine that we knew ahead of time that we were examining the skills of top chess players, and so changed our prior on all players to Normal(5, 1) and re-ran our approximate inference from scratch. Would that change the answer of either of the previous 2 questions, in expectation?"
      ],
      "metadata": {
        "id": "kjslZo7WUH2-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Your answer here."
      ],
      "metadata": {
        "id": "W37pO-PDNjKc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**k) [4 pts]** Based on all the plots and results in this assignment and HW2, which approximate inference method do you suspect is producing a better overall approximation to the true posterior over all skills conditioned on all games?  Give a short explanation."
      ],
      "metadata": {
        "id": "cNJ5WuyQKPPM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Your answer here."
      ],
      "metadata": {
        "id": "mtmXbA3OKSoM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. [21pts] Question 2: VAE with synthetic data\n",
        "\n",
        "In this question, we will train a VAE on a synthetic data which resembles spirals in 2d. We will perform amortized inference with VAEs."
      ],
      "metadata": {
        "id": "nDUMG4VJU4x5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Function below generates the synthetic knot shape data."
      ],
      "metadata": {
        "id": "S-NI6vCde9rg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def make_banana_data(num_classes, num_per_class, noise_std=0.3):\n",
        "    # Create points in the shape of a banana\n",
        "    features = []\n",
        "    labels = []\n",
        "\n",
        "    for i in range(num_classes):\n",
        "        theta = torch.linspace(0, np.pi, num_per_class) + i * (2 * np.pi / num_classes)  # spread across the circle\n",
        "        x = torch.sin(theta) + noise_std * torch.randn_like(theta)  # add some noise\n",
        "        y = torch.cos(theta) * (torch.sin(theta) + noise_std * torch.randn_like(theta))  # banana shape, noise too\n",
        "\n",
        "        features.append(torch.stack([x, y], dim=1))\n",
        "        labels.extend([i] * num_per_class)\n",
        "\n",
        "    # Concatenate features and labels\n",
        "    features = torch.cat(features, dim=0)\n",
        "    labels = torch.tensor(labels).long()\n",
        "\n",
        "    # Shuffle the dataset\n",
        "    perm_ix = torch.randperm(labels.shape[0])\n",
        "    return labels[perm_ix], features[perm_ix]"
      ],
      "metadata": {
        "id": "uyiFI2Aie9UH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We visualize the 2d data generated from the above function. Notice that there are 3 clusters in the input space, each colored with a different color. The VAE will not see the cluster assignments, but we hope to recover this structure in the latent space."
      ],
      "metadata": {
        "id": "_4HxOH1GWBno"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_clusters = 3\n",
        "samples_per_cluster = 300\n",
        "labels, data = make_banana_data(num_clusters, samples_per_cluster, 0.02)\n",
        "\n",
        "\n",
        "for k in range(num_clusters):\n",
        "    plt.scatter(data[labels == k, 0], data[labels == k, 1], s=2)\n",
        "\n",
        "plt.axis(\"equal\")"
      ],
      "metadata": {
        "id": "DUpKL-57dRR9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.1 [12 pts] Implement the missing lines in the below code, to complete the $\\texttt{elbo}$ function for a variational autoencoder.\n",
        "\n",
        "The banana dataset and an example encoder / decoder is provided just to help you debug.\n"
      ],
      "metadata": {
        "id": "QXlBa-8oFn9V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generic VAE functions.\n",
        "\n",
        "def log_prior(zs_array):\n",
        "    return diag_gaussian_log_density(zs_array, torch.tensor([0.0]), torch.tensor([1.0]))\n",
        "\n",
        "def diag_gaussian_samples(mean, log_std, num_samples):\n",
        "    return mean + torch.exp(log_std) * torch.randn(num_samples, mean.shape[-1])\n",
        "\n",
        "def diag_gaussian_logpdf(x, mean, log_std):\n",
        "    return diag_gaussian_log_density(x, mean, torch.exp(log_std))\n",
        "\n",
        "\n",
        "def batch_elbo(  # Simple Monte Carlo estimate of the variational lower bound.\n",
        "    recognition_net,    # takes a batch of datapoints, outputs mean and log_std of size (batch_size x latent_dim), i.e. log q(z|x)\n",
        "    decoder_net,        # takes a batch of latent samples, outputs mean and log_std of size (batch_size x data_dim), i.e. log p(x|z)\n",
        "    log_joint,          # takes decoder_net, a batch of latent samples, and a batch of datapoints, outputs unnormalized log joint, i.e. log p(x,z)\n",
        "    data                # a.k.a. x\n",
        "    ):\n",
        "    #TODO"
      ],
      "metadata": {
        "id": "ChZZaRmqhkvu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The below code trains a VAE where the encoder and decoder are both neural networks. The parameters are specified in the starter code.  You don't need to do anything here, this is just to help you debug."
      ],
      "metadata": {
        "id": "lOyXE8CCHNwT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "# Now define a specific VAE for the spiral dataset\n",
        "\n",
        "data_dimension = 2\n",
        "latent_dimension = 2\n",
        "\n",
        "# Define the recognition network.\n",
        "class RecognitionNet(nn.Module):\n",
        "    def __init__(self, data_dimension, latent_dimension):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(data_dimension, 100),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(100, 50),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.mean_net = nn.Linear(50, latent_dimension) # Output mean of q(z)\n",
        "        self.std_net = nn.Linear(50, latent_dimension) # Output log_std of q(z)\n",
        "\n",
        "    def forward(self, x):\n",
        "        interm = self.net(x)\n",
        "        mean, log_std = self.mean_net(interm), self.std_net(interm)\n",
        "        return mean, log_std\n",
        "\n",
        "recognition_net = RecognitionNet(data_dimension, latent_dimension)\n",
        "\n",
        "# Define the decoder network.\n",
        "# Note that it has two outputs, a mean and a variance, because\n",
        "# this model has a Gaussian likelihood p(x|z).\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, latent_dimension, data_dimension):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(latent_dimension, 100),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(100, 50),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.mean_net = nn.Linear(50, data_dimension) # Output mean of p(x|z)\n",
        "        self.std_net = nn.Linear(50, data_dimension) # Output log_std of p(x|z)\n",
        "\n",
        "    def forward(self, x):\n",
        "        interm = self.net(x)\n",
        "        mean, log_std = self.mean_net(interm), self.std_net(interm)\n",
        "        return mean, log_std\n",
        "\n",
        "decoder_net = Decoder(latent_dimension, data_dimension)\n",
        "\n",
        "# Set up log likelihood function.\n",
        "def log_likelihood(decoder_net, latent, data):\n",
        "    mean, log_std = decoder_net(latent)\n",
        "    return diag_gaussian_logpdf(data, mean,\n",
        "                                np.log(0.1) + 0. * log_std)  # Note: we are cheating here and using a fixed noise variance to make optimization more stable.\n",
        "\n",
        "def log_joint(decoder_net, latent, data):\n",
        "    return log_prior(latent) + log_likelihood(decoder_net, latent, data)\n",
        "\n",
        "# Run optimization\n",
        "optimizer = torch.optim.Adam([{'params': recognition_net.parameters()},\n",
        "                      {'params': decoder_net.parameters()}], lr=1e-3)\n",
        "n_iters = 2000\n",
        "minibatch_size = 300\n",
        "\n",
        "dataset = TensorDataset(torch.tensor(data))\n",
        "dataloader = DataLoader(dataset, batch_size=minibatch_size, shuffle=True)\n",
        "\n",
        "def objective(recognition_net, decoder_net):  # The loss function to be minimized.\n",
        "    minibatch = next(iter(dataloader))[0]\n",
        "    return -batch_elbo(\n",
        "    recognition_net,\n",
        "    decoder_net,\n",
        "    log_joint,\n",
        "    minibatch)\n",
        "\n",
        "def callback(t):\n",
        "    if t % 100 == 0:\n",
        "        print(\"Iteration {} lower bound {}\".format(t, -objective(recognition_net, decoder_net)))\n",
        "\n",
        "def update():\n",
        "    optimizer.zero_grad()\n",
        "    loss = objective(recognition_net, decoder_net)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "# Main loop.\n",
        "print(\"Optimizing variational parameters...\")\n",
        "for t in trange(0, n_iters):\n",
        "    update()\n",
        "    callback(t)"
      ],
      "metadata": {
        "id": "OgGgHfUsmid6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.2 [5 pts]  In this part, we visualize how the data looks like in the latent space. We simply use the trained recognition network (the encoder) to map each input to latent space.\n"
      ],
      "metadata": {
        "id": "ItI3GUaJH2o7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Show the means of the encoded data in a 2D latent space.\n",
        "# Don't worry if this doesn't look much like a Gaussian.\n",
        "\n",
        "for k in range(num_clusters):\n",
        "    # cur_data =  # TODO get all the data from this cluster.\n",
        "    # transformed =  # TODO find the mean of q(z|x) for each x.\n",
        "\n",
        "plt.axis(\"equal\")\n",
        "plt.xlabel(\"latent dimension 1\")\n",
        "plt.ylabel(\"latent dimension 2\")"
      ],
      "metadata": {
        "id": "u5GjmVm636GK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.3 [4pts] Generate new data using the decoder and the generative model we just trained.\n",
        "\n",
        "For this, we simply generate 1500 latent variables in the latent space from the prior and pass it through the decoder network.\n",
        "\n",
        "You shouldn't expect this to match the data exactly, just to get the overall shape and number of clusters roughly correct.  \n"
      ],
      "metadata": {
        "id": "FeSyYzKhINyK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample data from the trained generative model to see if it\n",
        "# roughly matches the data.  # Note: This doesn't add the likelihood noise,\n",
        "# although it should if we want it to match the data.\n",
        "\n",
        "num_samples = 1500\n",
        "# samples =  # TODO\n",
        "# transformed =  # TODO\n",
        "\n"
      ],
      "metadata": {
        "id": "YYm4QeiGk_Jm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here's a debugging tool only available when both the latent space and the data are both 2-dimensional.  We can show the function being learned by the encoder by showing how it warps a 2D grid into the latent space."
      ],
      "metadata": {
        "id": "Z3ndLm3JggYd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from matplotlib.collections import LineCollection\n",
        "\n",
        "def plot_grid(x,y, ax=None, **kwargs):\n",
        "    ax = ax or plt.gca()\n",
        "    segs1 = np.stack((x,y), axis=2)\n",
        "    segs2 = segs1.transpose(1,0,2)\n",
        "    ax.add_collection(LineCollection(segs1, **kwargs))\n",
        "    ax.add_collection(LineCollection(segs2, **kwargs))\n",
        "    ax.autoscale()\n",
        "\n",
        "def f(x,y):\n",
        "    xy = torch.stack([x.flatten(), y.flatten()], dim=1)\n",
        "    return decoder_net(xy)\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "\n",
        "grid_x,grid_y = torch.meshgrid(torch.linspace(-3,3,20),torch.linspace(-3,3,20))\n",
        "plot_grid(grid_x,grid_y, ax=ax,  color=\"lightgrey\")\n",
        "\n",
        "distx, disty = f(grid_x,grid_y)\n",
        "distx, disty = distx.reshape(20, 20, 2), disty.reshape(20, 20, 2)\n",
        "plot_grid(distx[:, :, 0].detach().numpy(), distx[:, :, 1].detach().numpy(), ax=ax, color=\"C0\")\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "UgdwiGrpsHRP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, we show the function being learned by the decoder by showing how it warps a 2D grid into the observed space.\n",
        "\n"
      ],
      "metadata": {
        "id": "qX7XKlJLgqFz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def f(x,y):\n",
        "    xy = torch.stack([x.flatten(), y.flatten()], dim=1)\n",
        "    return recognition_net(xy)\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "\n",
        "grid_x,grid_y = torch.meshgrid(torch.linspace(-3,3,20),torch.linspace(-3,3,20))\n",
        "plot_grid(grid_x,grid_y, ax=ax,  color=\"lightgrey\")\n",
        "\n",
        "distx, disty = f(grid_x,grid_y)\n",
        "distx, disty = distx.reshape(20, 20, 2), disty.reshape(20, 20, 2)\n",
        "plot_grid(distx[:, :, 0].detach().numpy(), distx[:, :, 1].detach().numpy(), ax=ax, color=\"C0\")\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "DqkriclvSsaN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
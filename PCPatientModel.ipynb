{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN8TeIJRhF7W8Z4QrkVKZ/P",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/javmencia/COBWEBfiles/blob/main/PCPatientModel.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ocDiLFe3YVjs",
        "outputId": "367f5c61-d925-4ab0-f485-393fd2afade8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLINICAL EARLY WARNING SYSTEM FOR MORTALITY PREDICTION\n",
            "======================================================================\n",
            "\n",
            "1. Loading data...\n",
            "   Loaded 8121 samples with 106 features\n",
            "======================================================================\n",
            "DATA DIAGNOSIS\n",
            "======================================================================\n",
            "✓ VISDAY: Found (8056 non-NA values)\n",
            "✓ os_event: Found (8121 non-NA values)\n",
            "✓ DSDAY: Found (1265 non-NA values)\n",
            "✓ RPT: Found (8121 non-NA values)\n",
            "\n",
            "Total records: 8121\n",
            "Unique patients (RPT): 526\n",
            "\n",
            "os_event distribution:\n",
            "  os_event = 0: 7023 records\n",
            "  os_event = 1: 1098 records\n",
            "  Missing os_event: 0 records\n",
            "\n",
            "Patients who die (os_event=1):\n",
            "  Count: 76\n",
            "\n",
            "Sample patient 4 (who dies):\n",
            "    VISDAY  DSDAY  os_event\n",
            "76     0.0  309.0         1\n",
            "78     8.0  309.0         1\n",
            "69    16.0  309.0         1\n",
            "70    29.0  309.0         1\n",
            "71    38.0  309.0         1\n",
            "72    58.0  309.0         1\n",
            "77    79.0  309.0         1\n",
            "73   155.0  309.0         1\n",
            "74   239.0  309.0         1\n",
            "75   309.0  309.0         1\n",
            "  Unique DSDAY values: 1 (non-NA: 1)\n",
            "  Death day: 309.0\n",
            "\n",
            "VISDAY vs DSDAY analysis:\n",
            "  Visits after death (VISDAY > DSDAY): 10\n",
            "  Visits on death day (VISDAY == DSDAY): 77\n",
            "  Visits before death (VISDAY < DSDAY): 1163\n",
            "\n",
            "======================================================================\n",
            "======================================================================\n",
            "PATIENT-LEVEL CLINICAL EARLY WARNING SYSTEM\n",
            "======================================================================\n",
            "Predicting patient-level mortality within 90 days\n",
            "\n",
            "1. Processing temporal data...\n",
            "Removed 65 rows with missing VISDAY\n",
            "Using 'RPT' as patient identifier\n",
            "\n",
            "DEBUG: Checking first few rows...\n",
            "   patient_id  VISDAY  DSDAY  os_event\n",
            "0           1     0.0    NaN         0\n",
            "1           1    11.0    NaN         0\n",
            "2           1    25.0    NaN         0\n",
            "3           1    35.0    NaN         0\n",
            "4           1    35.0    NaN         0\n",
            "5           1    35.0    NaN         0\n",
            "6           1    35.0    NaN         0\n",
            "7           1    58.0    NaN         0\n",
            "8           1    58.0    NaN         0\n",
            "9           1    79.0    NaN         0\n",
            "\n",
            "Calculating time-to-event for each visit...\n",
            "Processing 526 patients...\n",
            "  Processed 0/526 patients...\n",
            "\n",
            "DEBUG Patient 1:\n",
            "  Total visits: 33\n",
            "  os_event values: [0]\n",
            "  VISDAY range: 0.0 to 312.0\n",
            "  DSDAY values: [nan]\n",
            "  Non-NA DSDAY count: 0\n",
            "\n",
            "DEBUG Patient 2:\n",
            "  Total visits: 16\n",
            "  os_event values: [0]\n",
            "  VISDAY range: -25.0 to 277.0\n",
            "  DSDAY values: [nan]\n",
            "  Non-NA DSDAY count: 0\n",
            "\n",
            "DEBUG Patient 3:\n",
            "  Total visits: 20\n",
            "  os_event values: [0]\n",
            "  VISDAY range: 0.0 to 260.0\n",
            "  DSDAY values: [nan]\n",
            "  Non-NA DSDAY count: 0\n",
            "\n",
            "DEBUG Patient 4:\n",
            "  Total visits: 10\n",
            "  os_event values: [1]\n",
            "  VISDAY range: 0.0 to 309.0\n",
            "  DSDAY values: [309.]\n",
            "  Non-NA DSDAY count: 10\n",
            "  Patient 4: Death on day 309.0\n",
            "  Patient 4: 9 visits occur BEFORE death\n",
            "    First few future event calculations:\n",
            "      VISDAY=0.0, DSDAY=309.0, Days to death=309.0\n",
            "      VISDAY=8.0, DSDAY=309.0, Days to death=301.0\n",
            "      VISDAY=16.0, DSDAY=309.0, Days to death=293.0\n",
            "\n",
            "DEBUG Patient 5:\n",
            "  Total visits: 21\n",
            "  os_event values: [0]\n",
            "  VISDAY range: 0.0 to 116.0\n",
            "  DSDAY values: [nan]\n",
            "  Non-NA DSDAY count: 0\n",
            "\n",
            "DEBUG Patient 6:\n",
            "  Total visits: 10\n",
            "  os_event values: [0]\n",
            "  VISDAY range: 0.0 to 131.0\n",
            "  DSDAY values: [nan]\n",
            "  Non-NA DSDAY count: 0\n",
            "\n",
            "DEBUG Patient 7:\n",
            "  Total visits: 12\n",
            "  os_event values: [1]\n",
            "  VISDAY range: 0.0 to 199.0\n",
            "  DSDAY values: [199.]\n",
            "  Non-NA DSDAY count: 12\n",
            "  Patient 7: Death on day 199.0\n",
            "  Patient 7: 11 visits occur BEFORE death\n",
            "\n",
            "DEBUG Patient 8:\n",
            "  Total visits: 12\n",
            "  os_event values: [1]\n",
            "  VISDAY range: 0.0 to 216.0\n",
            "  DSDAY values: [301.]\n",
            "  Non-NA DSDAY count: 12\n",
            "  Patient 8: Death on day 301.0\n",
            "  Patient 8: 12 visits occur BEFORE death\n",
            "\n",
            "DEBUG Patient 9:\n",
            "  Total visits: 23\n",
            "  os_event values: [0]\n",
            "  VISDAY range: 5.0 to 469.0\n",
            "  DSDAY values: [nan]\n",
            "  Non-NA DSDAY count: 0\n",
            "\n",
            "DEBUG Patient 10:\n",
            "  Total visits: 8\n",
            "  os_event values: [0]\n",
            "  VISDAY range: 0.0 to 93.0\n",
            "  DSDAY values: [nan]\n",
            "  Non-NA DSDAY count: 0\n",
            "  Patient 24: Death on day 129.0\n",
            "  Patient 24: 4 visits occur BEFORE death\n",
            "  Patient 28: Death on day 208.0\n",
            "  Patient 28: 8 visits occur BEFORE death\n",
            "  Patient 29: Death on day 67.0\n",
            "  Patient 29: 6 visits occur BEFORE death\n",
            "  Patient 35: Death on day 206.0\n",
            "  Patient 35: 9 visits occur BEFORE death\n",
            "  Patient 41: Death on day 290.0\n",
            "  Patient 41: 15 visits occur BEFORE death\n",
            "  Patient 53: Death on day 320.0\n",
            "  Patient 53: 14 visits occur BEFORE death\n",
            "  Patient 56: Death on day 345.0\n",
            "  Patient 56: 33 visits occur BEFORE death\n",
            "  Patient 60: Death on day 207.0\n",
            "  Patient 60: 8 visits occur BEFORE death\n",
            "  Patient 61: Death on day 231.0\n",
            "  Patient 61: 13 visits occur BEFORE death\n",
            "  Patient 65: Death on day 228.0\n",
            "  Patient 65: 10 visits occur BEFORE death\n",
            "  Patient 67: Death on day 371.0\n",
            "  Patient 67: 17 visits occur BEFORE death\n",
            "  Patient 79: Death on day 203.0\n",
            "  Patient 79: 10 visits occur BEFORE death\n",
            "  Patient 81: Death on day 502.0\n",
            "  Patient 81: 18 visits occur BEFORE death\n",
            "  Patient 102: Death on day 280.0\n",
            "  Patient 102: 12 visits occur BEFORE death\n",
            "  Patient 108: Death on day 177.0\n",
            "  Patient 108: 13 visits occur BEFORE death\n",
            "  Patient 109: Death on day 238.0\n",
            "  Patient 109: 18 visits occur BEFORE death\n",
            "  Patient 112: Death on day 193.0\n",
            "  Patient 112: 6 visits occur BEFORE death\n",
            "  Patient 113: Death on day 471.0\n",
            "  Patient 113: 19 visits occur BEFORE death\n",
            "  Patient 125: Death on day 191.0\n",
            "  Patient 125: 4 visits occur BEFORE death\n",
            "  Patient 126: Death on day 199.0\n",
            "  Patient 126: 8 visits occur BEFORE death\n",
            "  Patient 129: Death on day 177.0\n",
            "  Patient 129: 7 visits occur BEFORE death\n",
            "  Patient 131: Death on day 381.0\n",
            "  Patient 131: 18 visits occur BEFORE death\n",
            "  Patient 133: Death on day 383.0\n",
            "  Patient 133: 19 visits occur BEFORE death\n",
            "  Patient 142: Death on day 375.0\n",
            "  Patient 142: 24 visits occur BEFORE death\n",
            "  Patient 145: Death on day 156.0\n",
            "  Patient 145: 8 visits occur BEFORE death\n",
            "  Patient 152: Death on day 536.0\n",
            "  Patient 152: 13 visits occur BEFORE death\n",
            "  Patient 158: Death on day 525.0\n",
            "  Patient 158: 16 visits occur BEFORE death\n",
            "  Patient 164: Death on day 408.0\n",
            "  Patient 164: 8 visits occur BEFORE death\n",
            "  Patient 173: Death on day 405.0\n",
            "  Patient 173: 21 visits occur BEFORE death\n",
            "  Patient 180: Death on day 504.0\n",
            "  Patient 180: 11 visits occur BEFORE death\n",
            "  Patient 194: Death on day 354.0\n",
            "  Patient 194: 18 visits occur BEFORE death\n",
            "  Patient 196: Death on day 104.0\n",
            "  Patient 196: 7 visits occur BEFORE death\n",
            "  Processed 200/526 patients...\n",
            "  Patient 202: Death on day 229.0\n",
            "  Patient 202: 11 visits occur BEFORE death\n",
            "  Patient 240: Death on day 443.0\n",
            "  Patient 240: 15 visits occur BEFORE death\n",
            "  Patient 241: Death on day 542.0\n",
            "  Patient 241: 17 visits occur BEFORE death\n",
            "  Patient 246: Death on day 165.0\n",
            "  Patient 246: 7 visits occur BEFORE death\n",
            "  Patient 248: Death on day 150.0\n",
            "  Patient 248: 8 visits occur BEFORE death\n",
            "  Patient 259: Death on day 240.0\n",
            "  Patient 259: 8 visits occur BEFORE death\n",
            "  Patient 263: Death on day 315.0\n",
            "  Patient 263: 11 visits occur BEFORE death\n",
            "  Patient 266: Death on day 523.0\n",
            "  Patient 266: 11 visits occur BEFORE death\n",
            "  Patient 284: Death on day 249.0\n",
            "  Patient 284: 8 visits occur BEFORE death\n",
            "  Patient 288: Death on day 274.0\n",
            "  Patient 288: 14 visits occur BEFORE death\n",
            "  Patient 293: Death on day 219.0\n",
            "  Patient 293: 8 visits occur BEFORE death\n",
            "  Patient 298: Death on day 116.0\n",
            "  Patient 298: 10 visits occur BEFORE death\n",
            "  Patient 307: Death on day 205.0\n",
            "  Patient 307: 14 visits occur BEFORE death\n",
            "  Patient 309: Death on day 231.0\n",
            "  Patient 309: 11 visits occur BEFORE death\n",
            "  Patient 312: Death on day 329.0\n",
            "  Patient 312: 12 visits occur BEFORE death\n",
            "  Patient 317: Death on day 397.0\n",
            "  Patient 317: 15 visits occur BEFORE death\n",
            "  Patient 321: Death on day 336.0\n",
            "  Patient 321: 17 visits occur BEFORE death\n",
            "  Patient 335: Death on day 346.0\n",
            "  Patient 335: 9 visits occur BEFORE death\n",
            "  Patient 338: Death on day 283.0\n",
            "  Patient 338: 14 visits occur BEFORE death\n",
            "  Patient 339: Death on day 405.0\n",
            "  Patient 339: 15 visits occur BEFORE death\n",
            "  Patient 341: Death on day 511.0\n",
            "  Patient 341: 20 visits occur BEFORE death\n",
            "  Patient 342: Death on day 226.0\n",
            "  Patient 342: 11 visits occur BEFORE death\n",
            "  Patient 368: Death on day 121.0\n",
            "  Patient 368: 8 visits occur BEFORE death\n",
            "  Patient 370: Death on day 240.0\n",
            "  Patient 370: 13 visits occur BEFORE death\n",
            "  Patient 371: Death on day 338.0\n",
            "  Patient 371: 15 visits occur BEFORE death\n",
            "  Patient 375: Death on day 511.0\n",
            "  Patient 375: 18 visits occur BEFORE death\n",
            "  Patient 386: Death on day 230.0\n",
            "  Patient 386: 10 visits occur BEFORE death\n",
            "  Patient 388: Death on day 223.0\n",
            "  Patient 388: 16 visits occur BEFORE death\n",
            "  Patient 398: Death on day 273.0\n",
            "  Patient 398: 15 visits occur BEFORE death\n",
            "  Patient 399: Death on day 135.0\n",
            "  Patient 399: 7 visits occur BEFORE death\n",
            "  Processed 400/526 patients...\n",
            "  Patient 402: Death on day 150.0\n",
            "  Patient 402: 5 visits occur BEFORE death\n",
            "  Patient 406: Death on day 326.0\n",
            "  Patient 406: 11 visits occur BEFORE death\n",
            "  Patient 430: Death on day 340.0\n",
            "  Patient 430: 34 visits occur BEFORE death\n",
            "  Patient 431: Death on day 353.0\n",
            "  Patient 431: 21 visits occur BEFORE death\n",
            "  Patient 445: Death on day 568.0\n",
            "  Patient 445: 23 visits occur BEFORE death\n",
            "  Patient 450: Death on day 318.0\n",
            "  Patient 450: 11 visits occur BEFORE death\n",
            "  Patient 456: Death on day 263.0\n",
            "  Patient 456: 13 visits occur BEFORE death\n",
            "  Patient 480: Death on day 367.0\n",
            "  Patient 480: 13 visits occur BEFORE death\n",
            "  Patient 482: Death on day 396.0\n",
            "  Patient 482: 33 visits occur BEFORE death\n",
            "  Patient 497: Death on day 212.0\n",
            "  Patient 497: 13 visits occur BEFORE death\n",
            "  Patient 501: Death on day 454.0\n",
            "  Patient 501: 17 visits occur BEFORE death\n",
            "\n",
            "PROCESSING COMPLETE:\n",
            "Total patients: 526\n",
            "Patients with os_event=1: 76\n",
            "Patients with DSDAY: 92\n",
            "Visits that occur BEFORE death (future events): 1009\n",
            "Percentage of visits with future events: 12.5%\n",
            "\n",
            "Time-to-event distribution for future events:\n",
            "  Mean: 220.9 days\n",
            "  Median: 203.0 days\n",
            "  Min: 5.0 days\n",
            "  Max: 568.0 days\n",
            "\n",
            "Patients with most future events:\n",
            "  Patient 430: 34 future events, Death day: 340.0\n",
            "  Patient 56: 33 future events, Death day: 345.0\n",
            "  Patient 482: 33 future events, Death day: 396.0\n",
            "  Patient 142: 24 future events, Death day: 375.0\n",
            "  Patient 445: 23 future events, Death day: 568.0\n",
            "\n",
            "DEBUG: Checking patient who dies (first one)...\n",
            "Patient 4:\n",
            "    VISDAY  DSDAY  os_event  time_to_event  is_future_event\n",
            "69     0.0  309.0         1          309.0                1\n",
            "70     8.0  309.0         1          301.0                1\n",
            "71    16.0  309.0         1          293.0                1\n",
            "72    29.0  309.0         1          280.0                1\n",
            "73    38.0  309.0         1          271.0                1\n",
            "74    58.0  309.0         1          251.0                1\n",
            "75    79.0  309.0         1          230.0                1\n",
            "76   155.0  309.0         1          154.0                1\n",
            "77   239.0  309.0         1           70.0                1\n",
            "78   309.0  309.0         1            0.0                0\n",
            "\n",
            "2. Selecting clinical features...\n",
            "Found 31 available features out of 50 possible\n",
            "Selected 31 features\n",
            "\n",
            "3. Creating patient-level sequences...\n",
            "Creating patient-level temporal sequences...\n",
            "\n",
            "Created 526 patient-level sequences\n",
            "Patients who eventually die: 76 (14.4%)\n",
            "Patients who survive: 450 (85.6%)\n",
            "Average visits per patient: 15.3\n",
            "Patient sequences shape: (526, 5, 31)\n",
            "Patient labels shape: (526,)\n",
            "\n",
            "4. Splitting data...\n",
            "Training patients: 294 (pos: 0.143)\n",
            "Validation patients: 74 (pos: 0.149)\n",
            "Test patients: 158 (pos: 0.146)\n",
            "\n",
            "5. Imputing missing values...\n",
            "Using autoencoder imputation: True\n",
            "Using 31 available features\n",
            "Categorical columns: 0\n",
            "Numeric columns: 31\n",
            "Train data shape before imputation: (1470, 31)\n",
            "Test data shape before imputation: (370, 31)\n",
            "Missing values in train: 868 (1.9%)\n",
            "Missing values in test: 216 (1.9%)\n",
            "\n",
            "Fitting autoencoder on training data...\n",
            "Autoencoder trained on 681 complete cases\n",
            "Imputing missing values in training set...\n",
            "Autoencoder imputation converged after 5 iterations\n",
            "Imputing missing values in test set...\n",
            "Autoencoder imputation converged after 5 iterations\n",
            "Missing values after autoencoder imputation (train): 0\n",
            "Missing values after autoencoder imputation (test): 0\n",
            "\n",
            "6. Training model...\n",
            "Model training completed\n",
            "\n",
            "7. Making predictions on test set...\n",
            "Using autoencoder imputation: True\n",
            "Using 31 available features\n",
            "Categorical columns: 0\n",
            "Numeric columns: 31\n",
            "Train data shape before imputation: (1470, 31)\n",
            "Test data shape before imputation: (790, 31)\n",
            "Missing values in train: 868 (1.9%)\n",
            "Missing values in test: 463 (1.9%)\n",
            "\n",
            "Fitting autoencoder on training data...\n",
            "Autoencoder trained on 681 complete cases\n",
            "Imputing missing values in training set...\n",
            "Autoencoder imputation converged after 5 iterations\n",
            "Imputing missing values in test set...\n",
            "Autoencoder imputation converged after 5 iterations\n",
            "Missing values after autoencoder imputation (train): 0\n",
            "Missing values after autoencoder imputation (test): 0\n",
            "Error: cannot reshape array of size 45570 into shape (158,5,31)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-3810321651.py\", line 1826, in run_clinical_early_warning\n",
            "    results = warning_pipeline.run_patient_level_pipeline(pcdata)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipython-input-3810321651.py\", line 1445, in run_patient_level_pipeline\n",
            "    X_test_imputed = X_test_imputed_flat.reshape(n_test_samples, seq_len, n_features)\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "ValueError: cannot reshape array of size 45570 into shape (158,5,31)\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc, precision_recall_curve, accuracy_score, roc_auc_score, precision_score, recall_score, f1_score\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, models, callbacks\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "class AutoencoderImputer:\n",
        "    def __init__(self, encoding_dim=32, epochs=50, batch_size=32, random_state=42):\n",
        "        self.encoding_dim = encoding_dim\n",
        "        self.epochs = epochs\n",
        "        self.batch_size = batch_size\n",
        "        self.random_state = random_state\n",
        "        self.scaler = StandardScaler()\n",
        "        self.autoencoder = None\n",
        "        self.is_fitted = False\n",
        "\n",
        "    def build_autoencoder(self, input_dim):\n",
        "        \"\"\"Build a deep autoencoder for imputation\"\"\"\n",
        "        input_layer = layers.Input(shape=(input_dim,))\n",
        "\n",
        "        # Encoder\n",
        "        encoded = layers.Dense(128, activation='relu')(input_layer)\n",
        "        encoded = layers.Dropout(0.2)(encoded)\n",
        "        encoded = layers.Dense(64, activation='relu')(encoded)\n",
        "        encoded = layers.Dropout(0.2)(encoded)\n",
        "        encoded = layers.Dense(self.encoding_dim, activation='relu')(encoded)\n",
        "\n",
        "        # Decoder\n",
        "        decoded = layers.Dense(64, activation='relu')(encoded)\n",
        "        decoded = layers.Dropout(0.2)(decoded)\n",
        "        decoded = layers.Dense(128, activation='relu')(decoded)\n",
        "        decoded = layers.Dropout(0.2)(decoded)\n",
        "        decoded = layers.Dense(input_dim, activation='linear')(decoded)\n",
        "\n",
        "        # Autoencoder model\n",
        "        autoencoder = models.Model(input_layer, decoded)\n",
        "\n",
        "        # Encoder model (for feature extraction)\n",
        "        encoder = models.Model(input_layer, encoded)\n",
        "\n",
        "        autoencoder.compile(optimizer=Adam(learning_rate=0.001),\n",
        "                           loss='mse',\n",
        "                           metrics=['mae'])\n",
        "\n",
        "        return autoencoder, encoder\n",
        "\n",
        "    def fit_imputer(self, X_train):\n",
        "        \"\"\"Train autoencoder on complete data\"\"\"\n",
        "        # Create mask for complete cases\n",
        "        complete_mask = ~np.any(np.isnan(X_train), axis=1)\n",
        "        X_complete = X_train[complete_mask]\n",
        "\n",
        "        if len(X_complete) < 10:\n",
        "            print(f\"Warning: Only {len(X_complete)} complete cases for autoencoder training\")\n",
        "            return self\n",
        "\n",
        "        # Scale the data\n",
        "        X_scaled = self.scaler.fit_transform(X_complete)\n",
        "\n",
        "        # Build and train autoencoder\n",
        "        self.autoencoder, self.encoder = self.build_autoencoder(X_scaled.shape[1])\n",
        "\n",
        "        early_stop = callbacks.EarlyStopping(\n",
        "            monitor='val_loss',\n",
        "            patience=10,\n",
        "            restore_best_weights=True,\n",
        "            min_delta=0.001\n",
        "        )\n",
        "\n",
        "        reduce_lr = callbacks.ReduceLROnPlateau(\n",
        "            monitor='val_loss',\n",
        "            factor=0.5,\n",
        "            patience=5,\n",
        "            min_lr=1e-6\n",
        "        )\n",
        "\n",
        "        # Train-validation split\n",
        "        X_train_split, X_val_split = train_test_split(\n",
        "            X_scaled, test_size=0.2, random_state=self.random_state\n",
        "        )\n",
        "\n",
        "        self.autoencoder.fit(\n",
        "            X_train_split, X_train_split,\n",
        "            epochs=self.epochs,\n",
        "            batch_size=self.batch_size,\n",
        "            validation_data=(X_val_split, X_val_split),\n",
        "            callbacks=[early_stop, reduce_lr],\n",
        "            verbose=0\n",
        "        )\n",
        "\n",
        "        self.is_fitted = True\n",
        "        print(f\"Autoencoder trained on {len(X_complete)} complete cases\")\n",
        "\n",
        "        return self\n",
        "\n",
        "    def impute_missing_values(self, X, max_iter=20, tol=1e-4):\n",
        "        \"\"\"Impute missing values using trained autoencoder with iterative refinement\"\"\"\n",
        "        if not self.is_fitted:\n",
        "            print(\"Autoencoder not fitted. Fitting now...\")\n",
        "            self.fit_imputer(X)\n",
        "            if not self.is_fitted:\n",
        "                return X  # Return original if still not fitted\n",
        "\n",
        "        X_imputed = X.copy()\n",
        "\n",
        "        # Check for missing values\n",
        "        if not np.any(np.isnan(X_imputed)):\n",
        "            return X_imputed\n",
        "\n",
        "        # Initial imputation with median\n",
        "        for i in range(X_imputed.shape[1]):\n",
        "            col_data = X_imputed[:, i]\n",
        "            if np.any(np.isnan(col_data)):\n",
        "                median_val = np.nanmedian(col_data)\n",
        "                if np.isnan(median_val):\n",
        "                    median_val = 0\n",
        "                col_data[np.isnan(col_data)] = median_val\n",
        "                X_imputed[:, i] = col_data\n",
        "\n",
        "        # Iterative refinement\n",
        "        for iteration in range(max_iter):\n",
        "            X_old = X_imputed.copy()\n",
        "\n",
        "            # Scale\n",
        "            X_scaled = self.scaler.transform(X_imputed)\n",
        "\n",
        "            # Get reconstructions\n",
        "            X_reconstructed = self.autoencoder.predict(X_scaled, verbose=0)\n",
        "            X_reconstructed = self.scaler.inverse_transform(X_reconstructed)\n",
        "\n",
        "            # Only update missing values\n",
        "            missing_mask = np.isnan(X)\n",
        "            X_imputed[missing_mask] = X_reconstructed[missing_mask]\n",
        "\n",
        "            # Check convergence\n",
        "            if iteration > 0:\n",
        "                change = np.linalg.norm(X_imputed - X_old) / np.linalg.norm(X_old)\n",
        "                if change < tol:\n",
        "                    print(f\"Autoencoder imputation converged after {iteration+1} iterations\")\n",
        "                    break\n",
        "\n",
        "        return X_imputed\n",
        "\n",
        "class RobustTimeAwareDataProcessor:\n",
        "    def __init__(self, use_autoencoder=True):\n",
        "        self.scaler = StandardScaler()\n",
        "        self.label_encoders = {}\n",
        "        self.use_autoencoder = use_autoencoder\n",
        "        self.autoencoder_imputer = AutoencoderImputer() if use_autoencoder else None\n",
        "        self.feature_names = None\n",
        "\n",
        "    def clean_data(self, df):\n",
        "        \"\"\"Clean data by handling infinity, large values, and missing data\"\"\"\n",
        "        df_clean = df.copy()\n",
        "\n",
        "        # Replace infinity with NaN\n",
        "        df_clean = df_clean.replace([np.inf, -np.inf], np.nan)\n",
        "\n",
        "        # Handle extremely large values by capping at 99th percentile\n",
        "        numeric_cols = df_clean.select_dtypes(include=[np.number]).columns\n",
        "        for col in numeric_cols:\n",
        "            if df_clean[col].notna().any():\n",
        "                # Cap at 99th percentile\n",
        "                p99 = df_clean[col].quantile(0.99)\n",
        "                if not np.isnan(p99):\n",
        "                    df_clean[col] = df_clean[col].clip(upper=p99 * 10)  # Allow some margin\n",
        "\n",
        "        return df_clean\n",
        "\n",
        "    def process_patient_data(self, df):\n",
        "        \"\"\"Process data with proper temporal structure using DSDAY for death timing\"\"\"\n",
        "        # Clean data first\n",
        "        df_clean = self.clean_data(df)\n",
        "\n",
        "        # Handle missing survival data\n",
        "        missing_before = len(df_clean)\n",
        "        df_clean = df_clean.dropna(subset=['VISDAY'])\n",
        "        missing_after = len(df_clean)\n",
        "        print(f\"Removed {missing_before - missing_after} rows with missing VISDAY\")\n",
        "\n",
        "        # Check for required columns\n",
        "        if 'DSDAY' not in df_clean.columns:\n",
        "            print(\"ERROR: DSDAY column not found. This is required for death timing.\")\n",
        "            print(\"Available columns:\", df_clean.columns.tolist())\n",
        "            return None\n",
        "\n",
        "        # Create patient IDs\n",
        "        if 'RPT' in df_clean.columns:\n",
        "            df_clean['patient_id'] = df_clean['RPT']\n",
        "            print(f\"Using 'RPT' as patient identifier\")\n",
        "        elif 'patient_id' not in df_clean.columns:\n",
        "            possible_id_cols = ['SUBJID', 'PATIENT', 'patient', 'id', 'ID']\n",
        "            for col in possible_id_cols:\n",
        "                if col in df_clean.columns:\n",
        "                    df_clean['patient_id'] = df_clean[col]\n",
        "                    print(f\"Using '{col}' as patient identifier\")\n",
        "                    break\n",
        "\n",
        "        # Sort by patient and visit day\n",
        "        df_sorted = df_clean.sort_values(['patient_id', 'VISDAY']).reset_index(drop=True)\n",
        "\n",
        "        # DEBUG: Check data structure\n",
        "        print(f\"\\nDEBUG: Checking first few rows...\")\n",
        "        print(df_sorted[['patient_id', 'VISDAY', 'DSDAY', 'os_event']].head(10))\n",
        "\n",
        "        # Calculate time to event for each visit\n",
        "        print(\"\\nCalculating time-to-event for each visit...\")\n",
        "        df_sorted['time_to_event'] = np.nan\n",
        "        df_sorted['is_future_event'] = 0\n",
        "\n",
        "        unique_patients = df_sorted['patient_id'].unique()\n",
        "        print(f\"Processing {len(unique_patients)} patients...\")\n",
        "\n",
        "        for i, pid in enumerate(unique_patients):\n",
        "            if i % 200 == 0:\n",
        "                print(f\"  Processed {i}/{len(unique_patients)} patients...\")\n",
        "\n",
        "            patient_data = df_sorted[df_sorted['patient_id'] == pid].copy()\n",
        "\n",
        "            # DEBUG for first few patients\n",
        "            if i < 10:\n",
        "                print(f\"\\nDEBUG Patient {pid}:\")\n",
        "                print(f\"  Total visits: {len(patient_data)}\")\n",
        "                print(f\"  os_event values: {patient_data['os_event'].unique()}\")\n",
        "                print(f\"  VISDAY range: {patient_data['VISDAY'].min()} to {patient_data['VISDAY'].max()}\")\n",
        "                print(f\"  DSDAY values: {patient_data['DSDAY'].unique()}\")\n",
        "                print(f\"  Non-NA DSDAY count: {patient_data['DSDAY'].notna().sum()}\")\n",
        "\n",
        "            # Check if patient died (has os_event = 1 AND has DSDAY)\n",
        "            patient_events = patient_data['os_event'].values\n",
        "            patient_dsdays = patient_data['DSDAY'].values\n",
        "\n",
        "            # Find non-NaN DSDAY values for this patient\n",
        "            valid_dsday_mask = ~pd.isna(patient_dsdays)\n",
        "\n",
        "            if np.any(patient_events == 1) and np.any(valid_dsday_mask):\n",
        "                # Patient died - get death day from DSDAY\n",
        "                # DSDAY should be consistent across all rows for a patient\n",
        "                death_day = patient_data.loc[valid_dsday_mask, 'DSDAY'].iloc[0]\n",
        "\n",
        "                print(f\"  Patient {pid}: Death on day {death_day}\")\n",
        "\n",
        "                # For each visit, check if it occurs before death\n",
        "                future_event_count = 0\n",
        "                for idx, row in patient_data.iterrows():\n",
        "                    if row['VISDAY'] < death_day:\n",
        "                        # Visit occurs BEFORE death\n",
        "                        days_to_death = death_day - row['VISDAY']\n",
        "                        df_sorted.at[idx, 'time_to_event'] = days_to_death\n",
        "                        df_sorted.at[idx, 'is_future_event'] = 1\n",
        "                        future_event_count += 1\n",
        "                    elif row['VISDAY'] == death_day:\n",
        "                        # Visit occurs ON death day\n",
        "                        df_sorted.at[idx, 'time_to_event'] = 0\n",
        "                        df_sorted.at[idx, 'is_future_event'] = 0\n",
        "                    elif row['VISDAY'] > death_day:\n",
        "                        # Visit occurs AFTER death (shouldn't happen with proper data)\n",
        "                        df_sorted.at[idx, 'time_to_event'] = -1  # Negative means after death\n",
        "                        df_sorted.at[idx, 'is_future_event'] = 0\n",
        "                    else:\n",
        "                        # Shouldn't reach here\n",
        "                        df_sorted.at[idx, 'time_to_event'] = np.nan\n",
        "                        df_sorted.at[idx, 'is_future_event'] = 0\n",
        "\n",
        "                print(f\"  Patient {pid}: {future_event_count} visits occur BEFORE death\")\n",
        "\n",
        "                # DEBUG: Show some time-to-event calculations\n",
        "                if i < 5 and future_event_count > 0:\n",
        "                    future_visits = patient_data[patient_data['VISDAY'] < death_day]\n",
        "                    print(f\"    First few future event calculations:\")\n",
        "                    for _, visit in future_visits.head(3).iterrows():\n",
        "                        print(f\"      VISDAY={visit['VISDAY']}, DSDAY={death_day}, Days to death={death_day - visit['VISDAY']}\")\n",
        "\n",
        "            else:\n",
        "                # Patient survived (os_event = 0 or missing DSDAY)\n",
        "                last_visit_day = patient_data['VISDAY'].max()\n",
        "\n",
        "                for idx, row in patient_data.iterrows():\n",
        "                    days_from_last = last_visit_day - row['VISDAY']\n",
        "                    df_sorted.at[idx, 'time_to_event'] = days_from_last\n",
        "                    df_sorted.at[idx, 'is_future_event'] = 0\n",
        "\n",
        "                if np.any(patient_events == 1) and not np.any(valid_dsday_mask):\n",
        "                    print(f\"  Patient {pid}: WARNING - os_event=1 but no DSDAY! Assuming censored.\")\n",
        "\n",
        "        # Summary statistics\n",
        "        print(f\"\\nPROCESSING COMPLETE:\")\n",
        "        print(f\"Total patients: {len(unique_patients)}\")\n",
        "\n",
        "        # Count patients with death events\n",
        "        death_patients = df_sorted[df_sorted['os_event'] == 1]['patient_id'].unique()\n",
        "        print(f\"Patients with os_event=1: {len(death_patients)}\")\n",
        "\n",
        "        # Count patients with valid DSDAY\n",
        "        patients_with_dsday = df_sorted[df_sorted['DSDAY'].notna()]['patient_id'].unique()\n",
        "        print(f\"Patients with DSDAY: {len(patients_with_dsday)}\")\n",
        "\n",
        "        # Count future events\n",
        "        total_future_events = df_sorted['is_future_event'].sum()\n",
        "        print(f\"Visits that occur BEFORE death (future events): {total_future_events}\")\n",
        "        print(f\"Percentage of visits with future events: {total_future_events/len(df_sorted)*100:.1f}%\")\n",
        "\n",
        "        # Show distribution of time_to_event for future events\n",
        "        future_events_data = df_sorted[df_sorted['is_future_event'] == 1]\n",
        "        if len(future_events_data) > 0:\n",
        "            print(f\"\\nTime-to-event distribution for future events:\")\n",
        "            print(f\"  Mean: {future_events_data['time_to_event'].mean():.1f} days\")\n",
        "            print(f\"  Median: {future_events_data['time_to_event'].median():.1f} days\")\n",
        "            print(f\"  Min: {future_events_data['time_to_event'].min():.1f} days\")\n",
        "            print(f\"  Max: {future_events_data['time_to_event'].max():.1f} days\")\n",
        "\n",
        "            # Show patients with the most future events\n",
        "            patient_future_counts = df_sorted.groupby('patient_id')['is_future_event'].sum()\n",
        "            top_patients = patient_future_counts.nlargest(5)\n",
        "            print(f\"\\nPatients with most future events:\")\n",
        "            for pid, count in top_patients.items():\n",
        "                patient_data = df_sorted[df_sorted['patient_id'] == pid]\n",
        "                death_day = patient_data['DSDAY'].iloc[0] if patient_data['DSDAY'].notna().any() else 'N/A'\n",
        "                print(f\"  Patient {pid}: {count} future events, Death day: {death_day}\")\n",
        "\n",
        "        # Debug: Check a specific patient\n",
        "        print(f\"\\nDEBUG: Checking patient who dies (first one)...\")\n",
        "        if len(death_patients) > 0:\n",
        "            sample_pid = death_patients[0]\n",
        "            sample_data = df_sorted[df_sorted['patient_id'] == sample_pid]\n",
        "            print(f\"Patient {sample_pid}:\")\n",
        "            print(sample_data[['VISDAY', 'DSDAY', 'os_event', 'time_to_event', 'is_future_event']].to_string())\n",
        "\n",
        "        return df_sorted\n",
        "\n",
        "    def prepare_features_with_autoencoder(self, df_train, df_test, feature_names):\n",
        "        \"\"\"Prepare features with autoencoder imputation for train and test separately\"\"\"\n",
        "        print(f\"Using autoencoder imputation: {self.use_autoencoder}\")\n",
        "\n",
        "        # Select features that exist in dataframe\n",
        "        available_features = [f for f in feature_names if f in df_train.columns]\n",
        "        print(f\"Using {len(available_features)} available features\")\n",
        "\n",
        "        # Prepare train and test data\n",
        "        X_train_raw = df_train[available_features].copy()\n",
        "        X_test_raw = df_test[available_features].copy()\n",
        "\n",
        "        # Replace any infinity values\n",
        "        X_train_raw = X_train_raw.replace([np.inf, -np.inf], np.nan)\n",
        "        X_test_raw = X_test_raw.replace([np.inf, -np.inf], np.nan)\n",
        "\n",
        "        # Handle categorical variables\n",
        "        categorical_cols = X_train_raw.select_dtypes(exclude=[np.number]).columns\n",
        "        numeric_cols = X_train_raw.select_dtypes(include=[np.number]).columns\n",
        "\n",
        "        print(f\"Categorical columns: {len(categorical_cols)}\")\n",
        "        print(f\"Numeric columns: {len(numeric_cols)}\")\n",
        "\n",
        "        # Encode categorical variables\n",
        "        encoded_train = []\n",
        "        encoded_test = []\n",
        "\n",
        "        for col in categorical_cols:\n",
        "            try:\n",
        "                if col not in self.label_encoders:\n",
        "                    self.label_encoders[col] = LabelEncoder()\n",
        "                    # Combine train and test for fitting to handle all categories\n",
        "                    combined = pd.concat([X_train_raw[col], X_test_raw[col]], ignore_index=True)\n",
        "                    combined = combined.astype(str).fillna('missing')\n",
        "                    self.label_encoders[col].fit(combined)\n",
        "\n",
        "                # Transform train and test\n",
        "                train_encoded = self.label_encoders[col].transform(\n",
        "                    X_train_raw[col].astype(str).fillna('missing')\n",
        "                )\n",
        "                test_encoded = self.label_encoders[col].transform(\n",
        "                    X_test_raw[col].astype(str).fillna('missing')\n",
        "                )\n",
        "\n",
        "                encoded_train.append(train_encoded.reshape(-1, 1))\n",
        "                encoded_test.append(test_encoded.reshape(-1, 1))\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Warning: Could not encode column {col}: {e}\")\n",
        "                # Drop problematic column\n",
        "                continue\n",
        "\n",
        "        # Combine categorical and numeric data\n",
        "        X_train_combined = []\n",
        "        X_test_combined = []\n",
        "\n",
        "        if len(categorical_cols) > 0 and encoded_train:\n",
        "            X_train_cat = np.hstack(encoded_train)\n",
        "            X_test_cat = np.hstack(encoded_test)\n",
        "            X_train_combined.append(X_train_cat)\n",
        "            X_test_combined.append(X_test_cat)\n",
        "\n",
        "        if len(numeric_cols) > 0:\n",
        "            X_train_num = X_train_raw[numeric_cols].values\n",
        "            X_test_num = X_test_raw[numeric_cols].values\n",
        "            X_train_combined.append(X_train_num)\n",
        "            X_test_combined.append(X_test_num)\n",
        "\n",
        "        # Combine all features\n",
        "        if X_train_combined:\n",
        "            X_train = np.hstack(X_train_combined)\n",
        "            X_test = np.hstack(X_test_combined)\n",
        "        else:\n",
        "            print(\"Warning: No features available after preprocessing\")\n",
        "            return None, None, available_features\n",
        "\n",
        "        print(f\"Train data shape before imputation: {X_train.shape}\")\n",
        "        print(f\"Test data shape before imputation: {X_test.shape}\")\n",
        "        print(f\"Missing values in train: {np.isnan(X_train).sum()} ({np.isnan(X_train).mean()*100:.1f}%)\")\n",
        "        print(f\"Missing values in test: {np.isnan(X_test).sum()} ({np.isnan(X_test).mean()*100:.1f}%)\")\n",
        "\n",
        "        if self.use_autoencoder and self.autoencoder_imputer:\n",
        "            # Fit autoencoder on training data\n",
        "            print(\"\\nFitting autoencoder on training data...\")\n",
        "            self.autoencoder_imputer.fit_imputer(X_train)\n",
        "\n",
        "            # Impute missing values separately for train and test\n",
        "            print(\"Imputing missing values in training set...\")\n",
        "            X_train_imputed = self.autoencoder_imputer.impute_missing_values(X_train)\n",
        "\n",
        "            print(\"Imputing missing values in test set...\")\n",
        "            X_test_imputed = self.autoencoder_imputer.impute_missing_values(X_test)\n",
        "\n",
        "            # Scale the imputed data\n",
        "            X_train_scaled = self.scaler.fit_transform(X_train_imputed)\n",
        "            X_test_scaled = self.scaler.transform(X_test_imputed)\n",
        "\n",
        "            print(f\"Missing values after autoencoder imputation (train): {np.isnan(X_train_scaled).sum()}\")\n",
        "            print(f\"Missing values after autoencoder imputation (test): {np.isnan(X_test_scaled).sum()}\")\n",
        "\n",
        "            return X_train_scaled, X_test_scaled, available_features\n",
        "        else:\n",
        "            # Fallback: Simple imputation\n",
        "            print(\"Using simple imputation (median/mode)...\")\n",
        "\n",
        "            # Impute missing values with median for numeric, mode for categorical\n",
        "            for i in range(X_train.shape[1]):\n",
        "                # Train imputation\n",
        "                train_col = X_train[:, i]\n",
        "                if np.any(np.isnan(train_col)):\n",
        "                    if i < len(categorical_cols):\n",
        "                        # Categorical - use mode\n",
        "                        mode_val = np.nanmode(train_col)\n",
        "                        if np.isnan(mode_val):\n",
        "                            mode_val = 0\n",
        "                        X_train[:, i] = np.nan_to_num(train_col, nan=mode_val)\n",
        "                    else:\n",
        "                        # Numeric - use median\n",
        "                        median_val = np.nanmedian(train_col)\n",
        "                        if np.isnan(median_val):\n",
        "                            median_val = 0\n",
        "                        X_train[:, i] = np.nan_to_num(train_col, nan=median_val)\n",
        "\n",
        "                # Test imputation (using statistics from train)\n",
        "                test_col = X_test[:, i]\n",
        "                if np.any(np.isnan(test_col)):\n",
        "                    if i < len(categorical_cols):\n",
        "                        # Categorical - use mode from train\n",
        "                        mode_val = np.nanmode(X_train[:, i])\n",
        "                        if np.isnan(mode_val):\n",
        "                            mode_val = 0\n",
        "                        X_test[:, i] = np.nan_to_num(test_col, nan=mode_val)\n",
        "                    else:\n",
        "                        # Numeric - use median from train\n",
        "                        median_val = np.nanmedian(X_train[:, i])\n",
        "                        if np.isnan(median_val):\n",
        "                            median_val = 0\n",
        "                        X_test[:, i] = np.nan_to_num(test_col, nan=median_val)\n",
        "\n",
        "            # Scale the data\n",
        "            X_train_scaled = self.scaler.fit_transform(X_train)\n",
        "            X_test_scaled = self.scaler.transform(X_test)\n",
        "\n",
        "            return X_train_scaled, X_test_scaled, available_features\n",
        "\n",
        "    def prepare_features(self, df, feature_names):\n",
        "        \"\"\"Legacy method for backward compatibility - prepares features without train/test split\"\"\"\n",
        "        print(\"Warning: Using legacy prepare_features method without train/test split\")\n",
        "\n",
        "        # Select features that exist in dataframe\n",
        "        available_features = [f for f in feature_names if f in df.columns]\n",
        "        print(f\"Using {len(available_features)} available features\")\n",
        "\n",
        "        # Handle missing values\n",
        "        df_filled = df[available_features].copy()\n",
        "\n",
        "        # Replace any remaining infinity values\n",
        "        df_filled = df_filled.replace([np.inf, -np.inf], np.nan)\n",
        "\n",
        "        # Fill numeric missing values with median\n",
        "        numeric_cols = df_filled.select_dtypes(include=[np.number]).columns\n",
        "        print(f\"Processing {len(numeric_cols)} numeric columns...\")\n",
        "\n",
        "        for col in numeric_cols:\n",
        "            if df_filled[col].isnull().any():\n",
        "                median_val = df_filled[col].median()\n",
        "                if pd.isna(median_val):\n",
        "                    median_val = 0  # Fallback\n",
        "                df_filled[col] = df_filled[col].fillna(median_val)\n",
        "\n",
        "        # Fill categorical missing values with mode\n",
        "        categorical_cols = df_filled.select_dtypes(exclude=[np.number]).columns\n",
        "        print(f\"Processing {len(categorical_cols)} categorical columns...\")\n",
        "\n",
        "        for col in categorical_cols:\n",
        "            if df_filled[col].isnull().any():\n",
        "                mode_val = df_filled[col].mode()\n",
        "                if len(mode_val) > 0:\n",
        "                    df_filled[col] = df_filled[col].fillna(mode_val[0])\n",
        "                else:\n",
        "                    df_filled[col] = df_filled[col].fillna('missing')\n",
        "\n",
        "        # Encode categorical variables\n",
        "        print(\"Encoding categorical variables...\")\n",
        "        for col in categorical_cols:\n",
        "            try:\n",
        "                if col not in self.label_encoders:\n",
        "                    self.label_encoders[col] = LabelEncoder()\n",
        "                    # Handle any non-string values\n",
        "                    col_data = df_filled[col].astype(str)\n",
        "                    self.label_encoders[col].fit(col_data)\n",
        "\n",
        "                df_filled[col] = self.label_encoders[col].transform(df_filled[col].astype(str))\n",
        "            except Exception as e:\n",
        "                print(f\"Warning: Could not encode column {col}: {e}\")\n",
        "                # Drop problematic column\n",
        "                df_filled = df_filled.drop(columns=[col])\n",
        "                available_features.remove(col)\n",
        "\n",
        "        # Scale numeric features\n",
        "        if len(numeric_cols) > 0:\n",
        "            print(\"Scaling numeric features...\")\n",
        "            # Check for any remaining issues\n",
        "            numeric_data = df_filled[numeric_cols].values\n",
        "            if np.any(np.isnan(numeric_data)) or np.any(np.isinf(numeric_data)):\n",
        "                print(\"Warning: NaN or Inf found in numeric data after cleaning\")\n",
        "                # Replace with zeros as last resort\n",
        "                numeric_data = np.nan_to_num(numeric_data, nan=0.0, posinf=1e6, neginf=-1e6)\n",
        "                df_filled[numeric_cols] = numeric_data\n",
        "\n",
        "            try:\n",
        "                df_filled[numeric_cols] = self.scaler.fit_transform(df_filled[numeric_cols])\n",
        "            except Exception as e:\n",
        "                print(f\"Warning: Could not scale features: {e}\")\n",
        "                # Use min-max scaling as fallback\n",
        "                for col in numeric_cols:\n",
        "                    col_min = df_filled[col].min()\n",
        "                    col_max = df_filled[col].max()\n",
        "                    if col_max > col_min:\n",
        "                        df_filled[col] = (df_filled[col] - col_min) / (col_max - col_min)\n",
        "                    else:\n",
        "                        df_filled[col] = 0\n",
        "\n",
        "        print(f\"Final feature matrix shape: {df_filled.shape}\")\n",
        "        return df_filled.values, available_features\n",
        "\n",
        "    def create_temporal_sequences(self, df, feature_cols, max_sequence_length=5, prediction_horizon=90):\n",
        "        \"\"\"Create sequences with proper temporal structure using DSDAY for death timing\"\"\"\n",
        "        print(\"Creating temporal sequences...\")\n",
        "        sequences = []\n",
        "        labels = []\n",
        "        time_intervals = []\n",
        "        patient_ids = []\n",
        "        days_to_event_list = []\n",
        "\n",
        "        unique_patients = df['patient_id'].unique()\n",
        "\n",
        "        for pid in unique_patients:\n",
        "            patient_data = df[df['patient_id'] == pid].sort_values('VISDAY')\n",
        "\n",
        "            if len(patient_data) < 2:\n",
        "                continue  # Need at least 2 visits\n",
        "\n",
        "            # Get features and temporal data\n",
        "            patient_features = patient_data[feature_cols].values\n",
        "            patient_visdays = patient_data['VISDAY'].values\n",
        "            patient_time_to_event = patient_data['time_to_event'].values\n",
        "            patient_has_event = patient_data['is_future_event'].values\n",
        "\n",
        "            # Create sequences\n",
        "            for i in range(1, len(patient_data)):  # Start from 1 to have at least one previous visit\n",
        "                # Determine sequence length (up to max_sequence_length)\n",
        "                seq_len = min(i, max_sequence_length)\n",
        "                start_idx = i - seq_len\n",
        "\n",
        "                # Get sequence data\n",
        "                seq_features = patient_features[start_idx:i+1]  # Including current visit\n",
        "                seq_visdays = patient_visdays[start_idx:i+1]\n",
        "\n",
        "                # Calculate time intervals between visits (in days)\n",
        "                if len(seq_visdays) > 1:\n",
        "                    time_diffs = np.diff(seq_visdays)\n",
        "                    # Add a default 30-day interval for the first gap\n",
        "                    time_diffs = np.concatenate([[30], time_diffs])\n",
        "                else:\n",
        "                    time_diffs = np.array([30])\n",
        "\n",
        "                # Normalize time intervals (divide by 30 to get approximate months)\n",
        "                time_diffs_normalized = time_diffs / 30.0\n",
        "\n",
        "                # Create label: will death occur within prediction horizon?\n",
        "                current_time_to_event = patient_time_to_event[i]\n",
        "                current_has_event = patient_has_event[i]\n",
        "\n",
        "                # Label is 1 if: this visit occurs before death AND death will happen within horizon\n",
        "                if current_has_event == 1 and current_time_to_event <= prediction_horizon:\n",
        "                    label = 1  # Death will occur within horizon\n",
        "                else:\n",
        "                    label = 0  # No death within horizon\n",
        "\n",
        "                # Pad sequence if needed\n",
        "                if seq_len < max_sequence_length:\n",
        "                    pad_len = max_sequence_length - seq_len\n",
        "                    seq_features = np.pad(seq_features, ((pad_len, 0), (0, 0)), mode='constant')\n",
        "                    time_diffs_normalized = np.pad(time_diffs_normalized, (pad_len, 0),\n",
        "                                                  mode='constant', constant_values=1.0)  # 1 month for padding\n",
        "\n",
        "                sequences.append(seq_features)\n",
        "                labels.append(label)\n",
        "                time_intervals.append(time_diffs_normalized)\n",
        "                patient_ids.append(pid)\n",
        "                days_to_event_list.append(current_time_to_event)\n",
        "\n",
        "        sequences = np.array(sequences)\n",
        "        labels = np.array(labels)\n",
        "        time_intervals = np.array(time_intervals)\n",
        "        patient_ids = np.array(patient_ids)\n",
        "\n",
        "        print(f\"\\nCreated {len(sequences)} sequences\")\n",
        "        print(f\"Positive sequences (death within {prediction_horizon} days): {np.sum(labels == 1)} ({np.mean(labels == 1)*100:.1f}%)\")\n",
        "        print(f\"Negative sequences: {np.sum(labels == 0)}\")\n",
        "\n",
        "        # Analyze positive sequences\n",
        "        if np.sum(labels == 1) > 0:\n",
        "            positive_days = np.array(days_to_event_list)[labels == 1]\n",
        "            print(f\"\\nDays to event for positive sequences (alerts):\")\n",
        "            print(f\"  Mean: {np.mean(positive_days):.1f} days\")\n",
        "            print(f\"  Median: {np.median(positive_days):.1f} days\")\n",
        "            print(f\"  Range: {np.min(positive_days):.1f} to {np.max(positive_days):.1f} days\")\n",
        "\n",
        "            # Show distribution\n",
        "            print(f\"  Distribution:\")\n",
        "            for threshold in [7, 14, 30, 60, 90]:\n",
        "                count = np.sum(positive_days <= threshold)\n",
        "                percentage = count / len(positive_days) * 100\n",
        "                print(f\"    ≤{threshold} days: {count} alerts ({percentage:.1f}%)\")\n",
        "\n",
        "        # Show some examples\n",
        "        print(f\"\\nDEBUG: Sample alerts (first 5):\")\n",
        "        alert_count = 0\n",
        "        for i in range(min(100, len(labels))):\n",
        "            if labels[i] == 1 and alert_count < 5:\n",
        "                pid = patient_ids[i]\n",
        "                days = days_to_event_list[i]\n",
        "                print(f\"  Patient {pid}: Alert at {days:.0f} days before death\")\n",
        "                alert_count += 1\n",
        "\n",
        "        return sequences, labels, time_intervals, patient_ids, days_to_event_list\n",
        "\n",
        "    def create_patient_level_sequences(self, df, feature_cols, max_sequence_length=5, prediction_horizon=90):\n",
        "        \"\"\"Create ONE sequence per patient with proper temporal structure\"\"\"\n",
        "        print(\"Creating patient-level temporal sequences...\")\n",
        "        sequences = []\n",
        "        patient_labels = []  # Patient-level labels (1 = dies eventually, 0 = survives)\n",
        "        time_intervals = []\n",
        "        patient_ids = []\n",
        "        patient_days_to_event = []  # Days to event at last visit\n",
        "        patient_visit_counts = []\n",
        "\n",
        "        unique_patients = df['patient_id'].unique()\n",
        "\n",
        "        for pid in unique_patients:\n",
        "            patient_data = df[df['patient_id'] == pid].sort_values('VISDAY')\n",
        "\n",
        "            if len(patient_data) < 1:\n",
        "                continue  # Need at least 1 visit\n",
        "\n",
        "            # Get features and temporal data\n",
        "            patient_features = patient_data[feature_cols].values\n",
        "            patient_visdays = patient_data['VISDAY'].values\n",
        "            patient_time_to_event = patient_data['time_to_event'].values\n",
        "            patient_has_event = patient_data['is_future_event'].values\n",
        "            patient_os_event = patient_data['os_event'].values\n",
        "\n",
        "            # Determine patient-level label: patient eventually dies (1) or survives (0)\n",
        "            patient_dies_eventually = 1 if np.any(patient_os_event == 1) else 0\n",
        "\n",
        "            # Use ALL available visits for the patient (up to max_sequence_length)\n",
        "            seq_len = min(len(patient_data), max_sequence_length)\n",
        "            start_idx = max(0, len(patient_data) - seq_len)\n",
        "\n",
        "            # Get sequence data (most recent visits)\n",
        "            seq_features = patient_features[start_idx:start_idx + seq_len]\n",
        "            seq_visdays = patient_visdays[start_idx:start_idx + seq_len]\n",
        "\n",
        "            # Calculate time intervals between visits (in days)\n",
        "            if len(seq_visdays) > 1:\n",
        "                time_diffs = np.diff(seq_visdays)\n",
        "                # Add a default 30-day interval for the first gap\n",
        "                time_diffs = np.concatenate([[30], time_diffs])\n",
        "            else:\n",
        "                time_diffs = np.array([30])\n",
        "\n",
        "            # Normalize time intervals\n",
        "            time_diffs_normalized = time_diffs / 30.0\n",
        "\n",
        "            # Pad sequence if needed\n",
        "            if seq_len < max_sequence_length:\n",
        "                pad_len = max_sequence_length - seq_len\n",
        "                seq_features = np.pad(seq_features, ((pad_len, 0), (0, 0)), mode='constant')\n",
        "                time_diffs_normalized = np.pad(time_diffs_normalized, (pad_len, 0),\n",
        "                                              mode='constant', constant_values=1.0)\n",
        "\n",
        "            sequences.append(seq_features)\n",
        "            patient_labels.append(patient_dies_eventually)\n",
        "            time_intervals.append(time_diffs_normalized)\n",
        "            patient_ids.append(pid)\n",
        "            patient_visit_counts.append(len(patient_data))\n",
        "\n",
        "            # Get days to event at the LAST visit\n",
        "            last_time_to_event = patient_time_to_event[-1]\n",
        "            patient_days_to_event.append(last_time_to_event)\n",
        "\n",
        "        sequences = np.array(sequences)\n",
        "        patient_labels = np.array(patient_labels)\n",
        "        time_intervals = np.array(time_intervals)\n",
        "        patient_ids = np.array(patient_ids)\n",
        "        patient_days_to_event = np.array(patient_days_to_event)  # Convert to numpy array\n",
        "        patient_visit_counts = np.array(patient_visit_counts)    # Convert to numpy array\n",
        "\n",
        "        print(f\"\\nCreated {len(sequences)} patient-level sequences\")\n",
        "        print(f\"Patients who eventually die: {np.sum(patient_labels == 1)} ({np.mean(patient_labels == 1)*100:.1f}%)\")\n",
        "        print(f\"Patients who survive: {np.sum(patient_labels == 0)} ({np.mean(patient_labels == 0)*100:.1f}%)\")\n",
        "        print(f\"Average visits per patient: {np.mean(patient_visit_counts):.1f}\")\n",
        "\n",
        "        return sequences, patient_labels, time_intervals, patient_ids, patient_days_to_event, patient_visit_counts\n",
        "\n",
        "class TimeAwareEarlyWarningModel:\n",
        "    def __init__(self, input_dim, sequence_length=5, lstm_units=64):\n",
        "        self.input_dim = input_dim\n",
        "        self.sequence_length = sequence_length\n",
        "        self.lstm_units = lstm_units\n",
        "        self.model = None\n",
        "        self.history = None\n",
        "\n",
        "    def build_model_with_time_awareness(self):\n",
        "        \"\"\"Build model that incorporates time intervals between visits\"\"\"\n",
        "        # Two input layers: features and time intervals\n",
        "        feature_input = layers.Input(shape=(self.sequence_length, self.input_dim), name='features')\n",
        "        time_input = layers.Input(shape=(self.sequence_length, 1), name='time_intervals')\n",
        "\n",
        "        # Process time intervals\n",
        "        time_processed = layers.Dense(8, activation='relu')(time_input)\n",
        "        time_processed = layers.Dense(16, activation='relu')(time_processed)\n",
        "\n",
        "        # Concatenate features with time information\n",
        "        combined = layers.Concatenate(axis=-1)([feature_input, time_processed])\n",
        "\n",
        "        # LSTM layers\n",
        "        lstm_out = layers.Bidirectional(layers.LSTM(self.lstm_units, return_sequences=True))(combined)\n",
        "        lstm_out = layers.Dropout(0.3)(lstm_out)\n",
        "\n",
        "        # Simple attention mechanism\n",
        "        attention_scores = layers.TimeDistributed(layers.Dense(1, activation='tanh'))(lstm_out)\n",
        "        attention_weights = layers.Softmax(axis=1)(attention_scores)\n",
        "        context_vector = layers.Multiply()([lstm_out, attention_weights])\n",
        "\n",
        "        # Global average pooling instead of just last time step\n",
        "        pooled = layers.GlobalAveragePooling1D()(context_vector)\n",
        "\n",
        "        # Dense layers\n",
        "        dense1 = layers.Dense(64, activation='relu')(pooled)\n",
        "        dense1 = layers.BatchNormalization()(dense1)\n",
        "        dense1 = layers.Dropout(0.3)(dense1)\n",
        "\n",
        "        dense2 = layers.Dense(32, activation='relu')(dense1)\n",
        "        dense2 = layers.BatchNormalization()(dense2)\n",
        "        dense2 = layers.Dropout(0.2)(dense2)\n",
        "\n",
        "        # Output: risk of event within prediction horizon\n",
        "        risk_score = layers.Dense(1, activation='sigmoid', name='risk_score')(dense2)\n",
        "\n",
        "        # Create model\n",
        "        self.model = models.Model(inputs=[feature_input, time_input], outputs=risk_score)\n",
        "\n",
        "        return self.model\n",
        "\n",
        "    def compile_model(self, learning_rate=0.001):\n",
        "        \"\"\"Compile the model\"\"\"\n",
        "        self.model.compile(\n",
        "            optimizer=Adam(learning_rate=learning_rate),\n",
        "            loss='binary_crossentropy',\n",
        "            metrics=['accuracy', tf.keras.metrics.AUC(name='auc'),\n",
        "                    tf.keras.metrics.Precision(name='precision'),\n",
        "                    tf.keras.metrics.Recall(name='recall')]\n",
        "        )\n",
        "\n",
        "        return self.model\n",
        "\n",
        "    def train(self, X_features, X_time, y_train, val_data=None, epochs=100, batch_size=32, class_weight=None):\n",
        "        \"\"\"Train the model\"\"\"\n",
        "        early_stopping = callbacks.EarlyStopping(\n",
        "            monitor='val_loss' if val_data else 'loss',\n",
        "            patience=15,\n",
        "            restore_best_weights=True,\n",
        "            min_delta=0.001,\n",
        "            verbose=0\n",
        "        )\n",
        "\n",
        "        reduce_lr = callbacks.ReduceLROnPlateau(\n",
        "            monitor='val_loss' if val_data else 'loss',\n",
        "            factor=0.5,\n",
        "            patience=8,\n",
        "            min_lr=1e-6,\n",
        "            verbose=0\n",
        "        )\n",
        "\n",
        "        callbacks_list = [early_stopping, reduce_lr]\n",
        "\n",
        "        if val_data:\n",
        "            X_val_features, X_val_time, y_val = val_data\n",
        "            validation_data = ([X_val_features, X_val_time], y_val)\n",
        "        else:\n",
        "            validation_data = None\n",
        "\n",
        "        self.history = self.model.fit(\n",
        "            [X_features, X_time], y_train,\n",
        "            validation_data=validation_data,\n",
        "            epochs=epochs,\n",
        "            batch_size=batch_size,\n",
        "            callbacks=callbacks_list,\n",
        "            verbose=0,\n",
        "            class_weight=class_weight\n",
        "        )\n",
        "\n",
        "        return self.history\n",
        "\n",
        "    def predict(self, X_features, X_time):\n",
        "        \"\"\"Make predictions\"\"\"\n",
        "        return self.model.predict([X_features, X_time], verbose=0).flatten()\n",
        "\n",
        "class ClinicalEarlyWarningPipeline:\n",
        "    def __init__(self, prediction_horizon_days=90, n_splits=3, random_state=42, use_autoencoder=True):\n",
        "        self.prediction_horizon = prediction_horizon_days\n",
        "        self.n_splits = n_splits\n",
        "        self.random_state = random_state\n",
        "        self.use_autoencoder = use_autoencoder\n",
        "        self.data_processor = RobustTimeAwareDataProcessor(use_autoencoder=use_autoencoder)\n",
        "        self.results = {}\n",
        "        self.optimal_thresholds = {}\n",
        "\n",
        "    def optimize_threshold_clinical(self, y_true, y_pred_proba, days_to_event=None, method='cost_sensitive'):\n",
        "        \"\"\"\n",
        "        Optimize threshold based on clinical utility rather than just F1 score\n",
        "        Methods:\n",
        "        - 'cost_sensitive': Balance FP vs FN based on clinical costs\n",
        "        - 'youden': Maximize Youden's J statistic (sensitivity + specificity - 1)\n",
        "        - 'closest_topleft': Closest point to (0,1) on ROC curve\n",
        "        - 'risk_stratified': Based on risk score distribution\n",
        "        - 'survival_weighted': Weight by time to event (earlier warnings more valuable)\n",
        "        \"\"\"\n",
        "\n",
        "        thresholds = np.linspace(0.05, 0.95, 181)  # More granular threshold search\n",
        "\n",
        "        if method == 'cost_sensitive':\n",
        "            # Clinical cost matrix: FN cost (missing a death) > FP cost (false alarm)\n",
        "            # In clinical settings, missing a death is typically 5-10x worse than false alarm\n",
        "            fn_cost = 5.0  # Cost of missing a death (false negative)\n",
        "            fp_cost = 1.0  # Cost of false alarm (false positive)\n",
        "\n",
        "            best_cost = float('inf')\n",
        "            best_threshold = 0.5\n",
        "\n",
        "            for thresh in thresholds:\n",
        "                y_pred = (y_pred_proba >= thresh).astype(int)\n",
        "\n",
        "                tn = np.sum((y_true == 0) & (y_pred == 0))\n",
        "                fp = np.sum((y_true == 0) & (y_pred == 1))\n",
        "                fn = np.sum((y_true == 1) & (y_pred == 0))\n",
        "                tp = np.sum((y_true == 1) & (y_pred == 1))\n",
        "\n",
        "                # Calculate total cost\n",
        "                total_cost = fp * fp_cost + fn * fn_cost\n",
        "\n",
        "                # Normalize by number of samples\n",
        "                normalized_cost = total_cost / len(y_true)\n",
        "\n",
        "                if normalized_cost < best_cost:\n",
        "                    best_cost = normalized_cost\n",
        "                    best_threshold = thresh\n",
        "\n",
        "            print(f\"Cost-sensitive optimal threshold: {best_threshold:.3f} (cost: {best_cost:.4f})\")\n",
        "            return best_threshold\n",
        "\n",
        "        elif method == 'youden':\n",
        "            # Maximize Youden's J statistic\n",
        "            best_j = -1\n",
        "            best_threshold = 0.5\n",
        "\n",
        "            for thresh in thresholds:\n",
        "                y_pred = (y_pred_proba >= thresh).astype(int)\n",
        "\n",
        "                tn = np.sum((y_true == 0) & (y_pred == 0))\n",
        "                fp = np.sum((y_true == 0) & (y_pred == 1))\n",
        "                fn = np.sum((y_true == 1) & (y_pred == 0))\n",
        "                tp = np.sum((y_true == 1) & (y_pred == 1))\n",
        "\n",
        "                sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
        "                specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
        "\n",
        "                j = sensitivity + specificity - 1\n",
        "\n",
        "                if j > best_j:\n",
        "                    best_j = j\n",
        "                    best_threshold = thresh\n",
        "\n",
        "            print(f\"Youden's J optimal threshold: {best_threshold:.3f} (J: {best_j:.4f})\")\n",
        "            return best_threshold\n",
        "\n",
        "        elif method == 'closest_topleft':\n",
        "            # Find threshold closest to (0,1) on ROC curve\n",
        "            fpr, tpr, thresh_vals = roc_curve(y_true, y_pred_proba)\n",
        "\n",
        "            # Calculate distance to (0,1)\n",
        "            distances = np.sqrt(fpr**2 + (1 - tpr)**2)\n",
        "            best_idx = np.argmin(distances)\n",
        "            best_threshold = thresh_vals[best_idx] if best_idx < len(thresh_vals) else 0.5\n",
        "\n",
        "            print(f\"Closest to top-left optimal threshold: {best_threshold:.3f}\")\n",
        "            return best_threshold\n",
        "\n",
        "        elif method == 'risk_stratified':\n",
        "            # Set threshold based on risk score distribution\n",
        "            # Aim for top X% of patients to be flagged as high risk\n",
        "            target_percentage_high_risk = 0.15  # Flag top 15% as high risk\n",
        "\n",
        "            sorted_scores = np.sort(y_pred_proba)\n",
        "            idx = int(len(sorted_scores) * (1 - target_percentage_high_risk))\n",
        "            best_threshold = sorted_scores[idx] if idx < len(sorted_scores) else 0.5\n",
        "\n",
        "            print(f\"Risk-stratified threshold: {best_threshold:.3f} (top {target_percentage_high_risk*100:.1f}%)\")\n",
        "            return best_threshold\n",
        "\n",
        "        elif method == 'survival_weighted' and days_to_event is not None:\n",
        "            # Weight by time to event - earlier warnings are more valuable\n",
        "            best_utility = -1\n",
        "            best_threshold = 0.5\n",
        "\n",
        "            for thresh in thresholds:\n",
        "                y_pred = (y_pred_proba >= thresh).astype(int)\n",
        "\n",
        "                # Calculate weighted utility\n",
        "                utility = 0\n",
        "\n",
        "                # For true positives, weight by how early the warning was\n",
        "                tp_mask = (y_true == 1) & (y_pred == 1)\n",
        "                if np.any(tp_mask):\n",
        "                    tp_days = days_to_event[tp_mask]\n",
        "                    # Early warnings get higher weight (more days to event)\n",
        "                    # Weight = 1 + (days_to_event / prediction_horizon)\n",
        "                    tp_weights = 1 + (tp_days / self.prediction_horizon)\n",
        "                    utility += np.sum(tp_weights)\n",
        "\n",
        "                # For false positives, penalize\n",
        "                fp_mask = (y_true == 0) & (y_pred == 1)\n",
        "                utility -= np.sum(fp_mask) * 0.5  # Lower penalty than missing a death\n",
        "\n",
        "                # For true negatives, small reward\n",
        "                tn_mask = (y_true == 0) & (y_pred == 0)\n",
        "                utility += np.sum(tn_mask) * 0.1\n",
        "\n",
        "                if utility > best_utility:\n",
        "                    best_utility = utility\n",
        "                    best_threshold = thresh\n",
        "\n",
        "            print(f\"Survival-weighted optimal threshold: {best_threshold:.3f} (utility: {best_utility:.2f})\")\n",
        "            return best_threshold\n",
        "\n",
        "        else:\n",
        "            # Default: Maximize F1 score\n",
        "            best_f1 = 0\n",
        "            best_threshold = 0.5\n",
        "\n",
        "            for thresh in thresholds:\n",
        "                y_pred = (y_pred_proba >= thresh).astype(int)\n",
        "                f1 = f1_score(y_true, y_pred, zero_division=0)\n",
        "\n",
        "                if f1 > best_f1:\n",
        "                    best_f1 = f1\n",
        "                    best_threshold = thresh\n",
        "\n",
        "            print(f\"F1-optimized threshold: {best_threshold:.3f} (F1: {best_f1:.4f})\")\n",
        "            return best_threshold\n",
        "\n",
        "    def find_optimal_threshold_ensemble(self, y_true, y_pred_proba, days_to_event=None):\n",
        "        \"\"\"\n",
        "        Use ensemble of methods to find robust optimal threshold\n",
        "        \"\"\"\n",
        "        methods = ['cost_sensitive', 'youden', 'closest_topleft', 'survival_weighted']\n",
        "        thresholds = []\n",
        "        weights = []\n",
        "\n",
        "        for method in methods:\n",
        "            try:\n",
        "                if method == 'survival_weighted' and days_to_event is not None:\n",
        "                    thresh = self.optimize_threshold_clinical(y_true, y_pred_proba, days_to_event, method)\n",
        "                else:\n",
        "                    thresh = self.optimize_threshold_clinical(y_true, y_pred_proba, method=method)\n",
        "                thresholds.append(thresh)\n",
        "\n",
        "                # Weight methods based on clinical relevance\n",
        "                if method == 'cost_sensitive':\n",
        "                    weights.append(0.4)  # Most important: clinical costs\n",
        "                elif method == 'survival_weighted':\n",
        "                    weights.append(0.3)  # Important: consider time to event\n",
        "                elif method == 'youden':\n",
        "                    weights.append(0.2)  # Standard metric\n",
        "                else:\n",
        "                    weights.append(0.1)  # Other methods\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Method {method} failed: {e}\")\n",
        "                continue\n",
        "\n",
        "        if thresholds:\n",
        "            # Weighted average of thresholds\n",
        "            weights = np.array(weights) / np.sum(weights)\n",
        "            optimal_threshold = np.average(thresholds, weights=weights)\n",
        "            print(f\"\\nEnsemble optimal threshold: {optimal_threshold:.3f}\")\n",
        "            print(f\"Individual thresholds: {[f'{t:.3f}' for t in thresholds]}\")\n",
        "            print(f\"Weights: {weights}\")\n",
        "\n",
        "            return optimal_threshold\n",
        "        else:\n",
        "            print(\"All threshold optimization methods failed, using default 0.5\")\n",
        "            return 0.5\n",
        "\n",
        "    def select_clinical_features(self, df):\n",
        "        \"\"\"Select relevant clinical features for early warning\"\"\"\n",
        "        # Common clinical features for mortality prediction\n",
        "        base_features = [\n",
        "            'AGE',  # Demographics\n",
        "            'ECOG'\n",
        "        ]\n",
        "\n",
        "\n",
        "        lab_features = [\n",
        "            'ALB', 'ALP', 'ALT', 'AST', 'BILI', 'CALCIUM', 'CREAT', 'HGB',\n",
        "            'LYMPH', 'NEUT', 'PLAT', 'WBC', 'SODIUM', 'POTASSIUM', 'GLUCOSE',\n",
        "            'BUN', 'CRP', 'LDH'\n",
        "        ]\n",
        "\n",
        "        treatment_features = [\n",
        "            'total_ae_events',           # 1. Overall AE burden\n",
        "            'serious_ae_count',          # 2. Serious AEs (strong mortality signal)\n",
        "            'grade3_plus_count',         # 3. High-grade toxicities\n",
        "            'drug_withdrawn',            # 4. Treatment discontinuation (critical event)\n",
        "            'any_treatment_related',     # 5. Treatment attribution (safety signal)\n",
        "            'cum_any_grade3_plus',        # 6. Cumulative high-grade toxicity burden\n",
        "            'max_ae_duration'\n",
        "            ]\n",
        "\n",
        "        # Treatment and AE features from your data\n",
        "        treatment_features = [\n",
        "            'cycle_number', 'total_cycles', 'anytarget', 'anynontarget',\n",
        "            'total_ae_events', 'serious_ae_count', 'grade3_plus_count',\n",
        "            'any_grade3_plus', 'treatment_related_count', 'any_treatment_related',\n",
        "            'grade1_count', 'grade2_count', 'grade3_count', 'grade4_count',\n",
        "            'drug_interrupted', 'drug_reduced', 'drug_withdrawn', 'drug_not_changed',\n",
        "            'other_drug_interrupted', 'other_drug_reduced', 'other_drug_withdrawn',\n",
        "            'concomitant_treatment_given', 'cum_any_grade3_plus',\n",
        "            'cum_any_treatment_related', 'cum_drug_withdrawn',\n",
        "            'cum_other_drug_withdrawn', 'cum_concomitant_treatment',\n",
        "            'ongoing_ae_count', 'mean_ae_duration', 'max_ae_duration'\n",
        "        ]\n",
        "\n",
        "        # Combine and check availability\n",
        "        all_possible = base_features + lab_features + treatment_features\n",
        "        available_features = [f for f in all_possible if f in df.columns]\n",
        "\n",
        "        print(f\"Found {len(available_features)} available features out of {len(all_possible)} possible\")\n",
        "\n",
        "        # If we have very few features, use some defaults\n",
        "        if len(available_features) < 10:\n",
        "            print(\"Warning: Few features available, using basic set\")\n",
        "            # Try to find any numeric columns\n",
        "            numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "            # Exclude ID and date columns\n",
        "            exclude_cols = ['VISDAY', 'DSDAY', 'patient_id', 'id', 'index', 'time_to_event', 'is_future_event']\n",
        "            numeric_cols = [c for c in numeric_cols if c not in exclude_cols]\n",
        "            available_features = numeric_cols[:20]  # Take first 20 numeric columns\n",
        "\n",
        "        return available_features\n",
        "\n",
        "\n",
        "    def run_simple_split(self, sequences, labels, time_intervals, patient_ids, days_to_event, df_processed, feature_names):\n",
        "        \"\"\"Run a simple train/test split when cross-validation fails\"\"\"\n",
        "        print(\"\\nRunning simple train/test split...\")\n",
        "\n",
        "        # Simple split by sequences (not patients)\n",
        "        n_samples = len(sequences)\n",
        "        indices = np.arange(n_samples)\n",
        "        np.random.shuffle(indices)\n",
        "\n",
        "        split_idx = int(0.8 * n_samples)\n",
        "        train_idx = indices[:split_idx]\n",
        "        test_idx = indices[split_idx:]\n",
        "\n",
        "        X_train_seq_raw = sequences[train_idx]\n",
        "        X_test_seq_raw = sequences[test_idx]\n",
        "\n",
        "        X_train_time = time_intervals[train_idx]\n",
        "        X_test_time = time_intervals[test_idx]\n",
        "\n",
        "        y_train = labels[train_idx]\n",
        "        y_test = labels[test_idx]\n",
        "\n",
        "        test_days_to_event = days_to_event[test_idx]\n",
        "\n",
        "        # Step: Impute missing values separately\n",
        "        print(\"\\nImputing missing values with autoencoder...\")\n",
        "\n",
        "        # Reshape sequences for imputation\n",
        "        n_train_samples, seq_len, n_features = X_train_seq_raw.shape\n",
        "        n_test_samples = X_test_seq_raw.shape[0]\n",
        "\n",
        "        X_train_flat = X_train_seq_raw.reshape(n_train_samples * seq_len, n_features)\n",
        "        X_test_flat = X_test_seq_raw.reshape(n_test_samples * seq_len, n_features)\n",
        "\n",
        "        # Create temporary dataframes\n",
        "        feature_cols = [f'feat_{i}' for i in range(n_features)]\n",
        "        df_train_impute = pd.DataFrame(X_train_flat, columns=feature_cols)\n",
        "        df_test_impute = pd.DataFrame(X_test_flat, columns=feature_cols)\n",
        "\n",
        "        # Impute missing values\n",
        "        X_train_imputed_flat, X_test_imputed_flat, _ = self.data_processor.prepare_features_with_autoencoder(\n",
        "            df_train_impute, df_test_impute, feature_cols\n",
        "        )\n",
        "\n",
        "        # Reshape back\n",
        "        X_train_seq_imputed = X_train_imputed_flat.reshape(n_train_samples, seq_len, n_features)\n",
        "        X_test_seq_imputed = X_test_imputed_flat.reshape(n_test_samples, seq_len, n_features)\n",
        "\n",
        "        print(f\"Train sequences after imputation: {X_train_seq_imputed.shape}\")\n",
        "        print(f\"Test sequences after imputation: {X_test_seq_imputed.shape}\")\n",
        "\n",
        "        # Split training for validation\n",
        "        val_size = int(0.2 * len(train_idx))\n",
        "        X_val_seq = X_train_seq_imputed[:val_size]\n",
        "        X_val_time = X_train_time[:val_size]\n",
        "        y_val = y_train[:val_size]\n",
        "        val_days_to_event = days_to_event[train_idx][:val_size]\n",
        "\n",
        "        X_train_seq_final = X_train_seq_imputed[val_size:]\n",
        "        X_train_time_final = X_train_time[val_size:]\n",
        "        y_train_final = y_train[val_size:]\n",
        "\n",
        "        print(f\"Training sequences: {X_train_seq_final.shape[0]} (pos: {np.mean(y_train_final):.3f})\")\n",
        "        print(f\"Validation sequences: {X_val_seq.shape[0]} (pos: {np.mean(y_val):.3f})\")\n",
        "        print(f\"Test sequences: {X_test_seq_imputed.shape[0]} (pos: {np.mean(y_test):.3f})\")\n",
        "\n",
        "        # Handle class imbalance\n",
        "        if np.sum(y_train_final) > 0:\n",
        "            pos_weight = len(y_train_final) / (2 * np.sum(y_train_final))\n",
        "            class_weight = {0: 1.0, 1: min(10.0, pos_weight)}  # Cap at 10x\n",
        "        else:\n",
        "            class_weight = {0: 1.0, 1: 1.0}\n",
        "\n",
        "        # Train model\n",
        "        print(\"\\nTraining time-aware early warning model...\")\n",
        "\n",
        "        model = TimeAwareEarlyWarningModel(\n",
        "            input_dim=X_train_seq_final.shape[2],\n",
        "            sequence_length=X_train_seq_final.shape[1],\n",
        "            lstm_units=32\n",
        "        )\n",
        "\n",
        "        model.build_model_with_time_awareness()\n",
        "        model.compile_model(learning_rate=0.001)\n",
        "\n",
        "        try:\n",
        "            history = model.train(\n",
        "                X_train_seq_final, X_train_time_final, y_train_final,\n",
        "                val_data=(X_val_seq, X_val_time, y_val),\n",
        "                epochs=50,\n",
        "                batch_size=32,\n",
        "                class_weight=class_weight\n",
        "            )\n",
        "\n",
        "            print(\"Training completed successfully\")\n",
        "        except Exception as e:\n",
        "            print(f\"Warning: Training failed: {e}\")\n",
        "            return None\n",
        "\n",
        "        # Evaluate\n",
        "        print(\"\\nEvaluating early warnings...\")\n",
        "\n",
        "        # Get predictions\n",
        "        try:\n",
        "            val_pred = model.predict(X_val_seq, X_val_time)\n",
        "            test_pred = model.predict(X_test_seq_imputed, X_test_time)\n",
        "        except Exception as e:\n",
        "            print(f\"Warning: Prediction failed: {e}\")\n",
        "            return None\n",
        "\n",
        "        # Find optimal threshold using ensemble method\n",
        "        optimal_threshold = self.find_optimal_threshold_ensemble(y_val, val_pred, val_days_to_event)\n",
        "\n",
        "        # Apply threshold to test set\n",
        "        test_pred_binary = (test_pred > optimal_threshold).astype(int)\n",
        "\n",
        "        if np.sum(test_pred_binary) == 0 or np.sum(y_test) == 0:\n",
        "            print(\"Warning: Insufficient predictions or true labels\")\n",
        "            return None\n",
        "\n",
        "        accuracy = accuracy_score(y_test, test_pred_binary)\n",
        "        try:\n",
        "            auc_score = roc_auc_score(y_test, test_pred)\n",
        "        except:\n",
        "            auc_score = 0.5\n",
        "        precision = precision_score(y_test, test_pred_binary, zero_division=0)\n",
        "        recall = recall_score(y_test, test_pred_binary, zero_division=0)\n",
        "        f1 = f1_score(y_test, test_pred_binary, zero_division=0)\n",
        "\n",
        "        # Calculate confusion matrix\n",
        "        cm = confusion_matrix(y_test, test_pred_binary)\n",
        "        if cm.shape == (2, 2):\n",
        "            tn, fp, fn, tp = cm.ravel()\n",
        "        else:\n",
        "            tn = np.sum((y_test == 0) & (test_pred_binary == 0))\n",
        "            fp = np.sum((y_test == 0) & (test_pred_binary == 1))\n",
        "            fn = np.sum((y_test == 1) & (test_pred_binary == 0))\n",
        "            tp = np.sum((y_test == 1) & (test_pred_binary == 1))\n",
        "\n",
        "        sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
        "        specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
        "\n",
        "        print(f\"\\nTest Results (Threshold: {optimal_threshold:.3f}):\")\n",
        "        print(f\"  Accuracy: {accuracy:.4f}\")\n",
        "        print(f\"  AUC: {auc_score:.4f}\")\n",
        "        print(f\"  Sensitivity: {sensitivity:.4f}\")\n",
        "        print(f\"  Specificity: {specificity:.4f}\")\n",
        "        print(f\"  Precision: {precision:.4f}\")\n",
        "        print(f\"  Recall: {recall:.4f}\")\n",
        "        print(f\"  F1-Score: {f1:.4f}\")\n",
        "        print(f\"  TP: {tp}, FP: {fp}, FN: {fn}, TN: {tn}\")\n",
        "\n",
        "        # Create results structure\n",
        "        results = {\n",
        "            'fold': [1],\n",
        "            'sensitivity': [sensitivity],\n",
        "            'specificity': [specificity],\n",
        "            'auc': [auc_score],\n",
        "            'precision': [precision],\n",
        "            'recall': [recall],\n",
        "            'f1': [f1],\n",
        "            'risk_scores': test_pred.tolist(),\n",
        "            'true_labels': y_test.tolist(),\n",
        "            'days_to_event': test_days_to_event.tolist(),\n",
        "            'warning_stats': [{\n",
        "                'correct_warnings': int(tp),\n",
        "                'missed_warnings': int(fn),\n",
        "                'false_warnings': int(fp),\n",
        "                'true_negatives': int(tn),\n",
        "                'avg_lead_time': np.mean(test_days_to_event[(y_test == 1) & (test_pred_binary == 1)]) if np.any((y_test == 1) & (test_pred_binary == 1)) else 0.0\n",
        "            }]\n",
        "        }\n",
        "\n",
        "        self.results = results\n",
        "\n",
        "        # Analyze results\n",
        "        print(\"\\n\" + \"=\"*70)\n",
        "        print(\"EARLY WARNING ANALYSIS (Simple Split)\")\n",
        "        print(\"=\"*70)\n",
        "\n",
        "        self.analyze_results(results)\n",
        "        self.visualize_results(results)\n",
        "\n",
        "        return results\n",
        "\n",
        "    def run_patient_level_pipeline(self, pcdata):\n",
        "        \"\"\"Run patient-level early warning pipeline\"\"\"\n",
        "        print(\"=\"*70)\n",
        "        print(\"PATIENT-LEVEL CLINICAL EARLY WARNING SYSTEM\")\n",
        "        print(\"=\"*70)\n",
        "        print(f\"Predicting patient-level mortality within {self.prediction_horizon} days\")\n",
        "\n",
        "        # Step 1: Process data\n",
        "        print(\"\\n1. Processing temporal data...\")\n",
        "        df_processed = self.data_processor.process_patient_data(pcdata)\n",
        "\n",
        "        if df_processed is None:\n",
        "            print(\"Error: Data processing failed\")\n",
        "            return None\n",
        "\n",
        "        # Step 2: Select features\n",
        "        print(\"\\n2. Selecting clinical features...\")\n",
        "        feature_names = self.select_clinical_features(df_processed)\n",
        "        print(f\"Selected {len(feature_names)} features\")\n",
        "\n",
        "        # Create feature columns in dataframe\n",
        "        feature_cols = [f'feat_{i}' for i in range(len(feature_names))]\n",
        "        for i, feat in enumerate(feature_names):\n",
        "            if feat in df_processed.columns:\n",
        "                df_processed[f'feat_{i}'] = df_processed[feat]\n",
        "\n",
        "        # Step 3: Create patient-level sequences\n",
        "        print(\"\\n3. Creating patient-level sequences...\")\n",
        "        patient_sequences, patient_labels, patient_time_intervals, patient_ids, \\\n",
        "        patient_days_to_event, patient_visit_counts = self.data_processor.create_patient_level_sequences(\n",
        "            df_processed, feature_cols, max_sequence_length=5,\n",
        "            prediction_horizon=self.prediction_horizon\n",
        "        )\n",
        "\n",
        "        if len(patient_sequences) == 0:\n",
        "            print(\"Error: No patient sequences created!\")\n",
        "            return None\n",
        "\n",
        "        print(f\"Patient sequences shape: {patient_sequences.shape}\")\n",
        "        print(f\"Patient labels shape: {patient_labels.shape}\")\n",
        "\n",
        "        # Step 4: Split data for training and testing\n",
        "        print(\"\\n4. Splitting data...\")\n",
        "\n",
        "        # Convert lists to numpy arrays for indexing\n",
        "        patient_days_to_event = np.array(patient_days_to_event)\n",
        "        patient_visit_counts = np.array(patient_visit_counts)\n",
        "\n",
        "        # Use patient-level split\n",
        "        X = patient_sequences\n",
        "        y = patient_labels\n",
        "        time_intervals = patient_time_intervals\n",
        "        pids = patient_ids\n",
        "\n",
        "        # Split into train and test\n",
        "        train_idx, test_idx = train_test_split(\n",
        "            np.arange(len(X)),\n",
        "            test_size=0.3,\n",
        "            stratify=y if np.unique(y).size > 1 else None,\n",
        "            random_state=self.random_state\n",
        "        )\n",
        "\n",
        "        X_train = X[train_idx]\n",
        "        X_test = X[test_idx]\n",
        "        X_train_time = time_intervals[train_idx]\n",
        "        X_test_time = time_intervals[test_idx]\n",
        "        y_train = y[train_idx]\n",
        "        y_test = y[test_idx]\n",
        "        test_pids = pids[test_idx]\n",
        "        test_days_to_event = patient_days_to_event[test_idx]\n",
        "        test_visit_counts = patient_visit_counts[test_idx]\n",
        "\n",
        "        # Further split training for validation\n",
        "        train_idx, val_idx = train_test_split(\n",
        "            np.arange(len(X_train)),\n",
        "            test_size=0.2,\n",
        "            stratify=y_train if np.unique(y_train).size > 1 else None,\n",
        "            random_state=self.random_state\n",
        "        )\n",
        "\n",
        "        X_train_final = X_train[train_idx]\n",
        "        X_val = X_train[val_idx]\n",
        "        X_train_time_final = X_train_time[train_idx]\n",
        "        X_val_time = X_train_time[val_idx]\n",
        "        y_train_final = y_train[train_idx]\n",
        "        y_val = y_train[val_idx]\n",
        "\n",
        "        print(f\"Training patients: {len(X_train_final)} (pos: {np.mean(y_train_final):.3f})\")\n",
        "        print(f\"Validation patients: {len(X_val)} (pos: {np.mean(y_val):.3f})\")\n",
        "        print(f\"Test patients: {len(X_test)} (pos: {np.mean(y_test):.3f})\")\n",
        "\n",
        "        # Step 5: Impute missing values\n",
        "        print(\"\\n5. Imputing missing values...\")\n",
        "\n",
        "        # Impute training data\n",
        "        n_train_samples, seq_len, n_features = X_train_final.shape\n",
        "        X_train_flat = X_train_final.reshape(n_train_samples * seq_len, n_features)\n",
        "        df_train_impute = pd.DataFrame(X_train_flat, columns=feature_cols)\n",
        "\n",
        "        # Use a copy for validation\n",
        "        df_val_impute = pd.DataFrame(X_val.reshape(-1, n_features), columns=feature_cols)\n",
        "\n",
        "        X_train_imputed_flat, X_val_imputed_flat, _ = self.data_processor.prepare_features_with_autoencoder(\n",
        "            df_train_impute, df_val_impute, feature_cols\n",
        "        )\n",
        "\n",
        "        X_train_imputed = X_train_imputed_flat.reshape(n_train_samples, seq_len, n_features)\n",
        "        X_val_imputed = X_val_imputed_flat.reshape(X_val.shape[0], seq_len, n_features)\n",
        "\n",
        "        # Step 6: Train model\n",
        "        print(\"\\n6. Training model...\")\n",
        "\n",
        "        # Handle class imbalance\n",
        "        if np.sum(y_train_final) > 0:\n",
        "            pos_weight = len(y_train_final) / (2 * np.sum(y_train_final))\n",
        "            class_weight = {0: 1.0, 1: min(10.0, pos_weight)}\n",
        "        else:\n",
        "            class_weight = {0: 1.0, 1: 1.0}\n",
        "\n",
        "        model = TimeAwareEarlyWarningModel(\n",
        "            input_dim=n_features,\n",
        "            sequence_length=seq_len,\n",
        "            lstm_units=32\n",
        "        )\n",
        "\n",
        "        model.build_model_with_time_awareness()\n",
        "        model.compile_model(learning_rate=0.001)\n",
        "\n",
        "        try:\n",
        "            history = model.train(\n",
        "                X_train_imputed, X_train_time_final, y_train_final,\n",
        "                val_data=(X_val_imputed, X_val_time, y_val),\n",
        "                epochs=50,\n",
        "                batch_size=32,\n",
        "                class_weight=class_weight\n",
        "            )\n",
        "            print(\"Model training completed\")\n",
        "        except Exception as e:\n",
        "            print(f\"Training failed: {e}\")\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "            return None\n",
        "\n",
        "        # Step 7: Make predictions on test set\n",
        "        print(\"\\n7. Making predictions on test set...\")\n",
        "\n",
        "        # Impute test data\n",
        "        n_test_samples = X_test.shape[0]\n",
        "        X_test_flat = X_test.reshape(n_test_samples * seq_len, n_features)\n",
        "        df_test_impute = pd.DataFrame(X_test_flat, columns=feature_cols)\n",
        "\n",
        "        # Use training data for fitting imputer\n",
        "        X_test_imputed_flat, _, _ = self.data_processor.prepare_features_with_autoencoder(\n",
        "            df_train_impute, df_test_impute, feature_cols\n",
        "        )\n",
        "\n",
        "        X_test_imputed = X_test_imputed_flat.reshape(n_test_samples, seq_len, n_features)\n",
        "\n",
        "        # Get predictions\n",
        "        test_pred_proba = model.predict(X_test_imputed, X_test_time)\n",
        "\n",
        "        # Step 8: Find optimal threshold\n",
        "        print(\"\\n8. Optimizing threshold...\")\n",
        "        optimal_threshold = self.find_optimal_threshold_ensemble(y_val, test_pred_proba[:len(y_val)])\n",
        "\n",
        "        # Apply threshold\n",
        "        test_pred_binary = (test_pred_proba > optimal_threshold).astype(int)\n",
        "\n",
        "        # Step 9: Analyze results\n",
        "        results = self.analyze_patient_level_results(\n",
        "            test_pids, y_test, test_pred_proba, test_pred_binary,\n",
        "            test_days_to_event, test_visit_counts, optimal_threshold\n",
        "        )\n",
        "\n",
        "        # Also make predictions on all patients for final output\n",
        "        print(\"\\n9. Making final predictions on all patients...\")\n",
        "\n",
        "        # Impute all patient data\n",
        "        n_all_samples = patient_sequences.shape[0]\n",
        "        all_flat = patient_sequences.reshape(n_all_samples * seq_len, n_features)\n",
        "        df_all_impute = pd.DataFrame(all_flat, columns=feature_cols)\n",
        "\n",
        "        all_imputed_flat, _, _ = self.data_processor.prepare_features_with_autoencoder(\n",
        "            df_train_impute, df_all_impute, feature_cols\n",
        "        )\n",
        "\n",
        "        all_imputed = all_imputed_flat.reshape(n_all_samples, seq_len, n_features)\n",
        "\n",
        "        # Predict on all patients\n",
        "        all_pred_proba = model.predict(all_imputed, patient_time_intervals)\n",
        "        all_pred_binary = (all_pred_proba > optimal_threshold).astype(int)\n",
        "\n",
        "        # Add all predictions to results\n",
        "        results['all_patient_ids'] = patient_ids.tolist()\n",
        "        results['all_predicted_risk'] = all_pred_proba.tolist()\n",
        "        results['all_predicted_label'] = all_pred_binary.tolist()\n",
        "\n",
        "        return results\n",
        "\n",
        "\n",
        "\n",
        "    def analyze_patient_level_results(self, patient_ids, true_labels, pred_proba, pred_binary,\n",
        "                                     days_to_event, visit_counts, threshold):\n",
        "        \"\"\"Analyze patient-level prediction results\"\"\"\n",
        "        results = {\n",
        "            'patient_id': patient_ids.tolist(),\n",
        "            'true_label': true_labels.tolist(),\n",
        "            'predicted_risk': pred_proba.tolist(),\n",
        "            'predicted_label': pred_binary.tolist(),\n",
        "            'days_to_event': days_to_event.tolist(),\n",
        "            'visit_count': visit_counts.tolist(),\n",
        "            'threshold_used': threshold\n",
        "        }\n",
        "\n",
        "        print(\"\\n\" + \"=\"*70)\n",
        "        print(\"PATIENT-LEVEL PREDICTION RESULTS\")\n",
        "        print(\"=\"*70)\n",
        "\n",
        "        # Calculate metrics\n",
        "        accuracy = accuracy_score(true_labels, pred_binary)\n",
        "        try:\n",
        "            auc_score = roc_auc_score(true_labels, pred_proba)\n",
        "        except:\n",
        "            auc_score = 0.5\n",
        "        precision = precision_score(true_labels, pred_binary, zero_division=0)\n",
        "        recall = recall_score(true_labels, pred_binary, zero_division=0)\n",
        "        f1 = f1_score(true_labels, pred_binary, zero_division=0)\n",
        "\n",
        "        # Confusion matrix\n",
        "        cm = confusion_matrix(true_labels, pred_binary)\n",
        "        if cm.shape == (2, 2):\n",
        "            tn, fp, fn, tp = cm.ravel()\n",
        "        else:\n",
        "            tn = np.sum((true_labels == 0) & (pred_binary == 0))\n",
        "            fp = np.sum((true_labels == 0) & (pred_binary == 1))\n",
        "            fn = np.sum((true_labels == 1) & (pred_binary == 0))\n",
        "            tp = np.sum((true_labels == 1) & (pred_binary == 1))\n",
        "\n",
        "        sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
        "        specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
        "\n",
        "        print(f\"\\nPERFORMANCE METRICS (Threshold: {threshold:.3f}):\")\n",
        "        print(\"-\"*50)\n",
        "        print(f\"Accuracy:    {accuracy:.4f}\")\n",
        "        print(f\"AUC:         {auc_score:.4f}\")\n",
        "        print(f\"Sensitivity: {sensitivity:.4f}\")\n",
        "        print(f\"Specificity: {specificity:.4f}\")\n",
        "        print(f\"Precision:   {precision:.4f}\")\n",
        "        print(f\"Recall:      {recall:.4f}\")\n",
        "        print(f\"F1-Score:    {f1:.4f}\")\n",
        "        print(f\"\\nConfusion Matrix:\")\n",
        "        print(f\"True Positives:  {tp}\")\n",
        "        print(f\"False Positives: {fp}\")\n",
        "        print(f\"False Negatives: {fn}\")\n",
        "        print(f\"True Negatives:  {tn}\")\n",
        "\n",
        "        # Patient-level analysis\n",
        "        print(f\"\\nPATIENT ANALYSIS:\")\n",
        "        print(\"-\"*50)\n",
        "        print(f\"Total patients: {len(patient_ids)}\")\n",
        "        print(f\"Patients who died: {np.sum(true_labels == 1)}\")\n",
        "        print(f\"Patients predicted high risk: {np.sum(pred_binary == 1)}\")\n",
        "        print(f\"Average visits per patient: {np.mean(visit_counts):.1f}\")\n",
        "\n",
        "        # High-risk patients analysis\n",
        "        high_risk_mask = pred_binary == 1\n",
        "        if np.any(high_risk_mask):\n",
        "            high_risk_patients = patient_ids[high_risk_mask]\n",
        "            high_risk_true = true_labels[high_risk_mask]\n",
        "            high_risk_accuracy = np.mean(high_risk_true == 1)\n",
        "\n",
        "            print(f\"\\nHIGH-RISK PATIENTS ({np.sum(high_risk_mask)}):\")\n",
        "            print(f\"  Correctly identified: {np.sum(high_risk_true == 1)} ({high_risk_accuracy*100:.1f}%)\")\n",
        "            print(f\"  Average risk score: {np.mean(pred_proba[high_risk_mask]):.3f}\")\n",
        "\n",
        "            # Show some high-risk patients\n",
        "            print(f\"\\n  Example high-risk patients:\")\n",
        "            for i in range(min(5, len(high_risk_patients))):\n",
        "                pid = high_risk_patients[i]\n",
        "                risk = pred_proba[high_risk_mask][i]\n",
        "                actual = high_risk_true[i]\n",
        "                days = days_to_event[high_risk_mask][i]\n",
        "                visits = visit_counts[high_risk_mask][i]\n",
        "                status = \"DIED\" if actual == 1 else \"SURVIVED\"\n",
        "                print(f\"    Patient {pid}: Risk={risk:.3f}, {status}, {visits} visits\")\n",
        "\n",
        "        # Create visualization\n",
        "        self.visualize_patient_level_results(results)\n",
        "\n",
        "        return results\n",
        "\n",
        "    def visualize_patient_level_results(self, results):\n",
        "        \"\"\"Visualize patient-level results\"\"\"\n",
        "        try:\n",
        "            fig = plt.figure(figsize=(20, 12))\n",
        "\n",
        "            # 1. Risk Score Distribution\n",
        "            ax1 = plt.subplot(2, 3, 1)\n",
        "            risk_scores = np.array(results['predicted_risk'])\n",
        "            true_labels = np.array(results['true_label'])\n",
        "\n",
        "            event_mask = true_labels == 1\n",
        "            no_event_mask = true_labels == 0\n",
        "\n",
        "            ax1.hist(risk_scores[no_event_mask], bins=30, alpha=0.7,\n",
        "                    label='Survived', color='blue', edgecolor='black', density=True)\n",
        "            ax1.hist(risk_scores[event_mask], bins=30, alpha=0.7,\n",
        "                    label='Died', color='red', edgecolor='black', density=True)\n",
        "\n",
        "            # Mark threshold\n",
        "            threshold = results['threshold_used']\n",
        "            ax1.axvline(x=threshold, color='black', linestyle='--',\n",
        "                       label=f'Threshold ({threshold:.3f})', linewidth=2)\n",
        "\n",
        "            ax1.set_xlabel('Risk Score')\n",
        "            ax1.set_ylabel('Density')\n",
        "            ax1.set_title('Risk Score Distribution by Outcome')\n",
        "            ax1.legend()\n",
        "            ax1.grid(True, alpha=0.3)\n",
        "\n",
        "            # 2. ROC Curve\n",
        "            ax2 = plt.subplot(2, 3, 2)\n",
        "            fpr, tpr, _ = roc_curve(true_labels, risk_scores)\n",
        "            roc_auc = auc(fpr, tpr)\n",
        "\n",
        "            ax2.plot(fpr, tpr, color='darkorange', lw=2,\n",
        "                    label=f'ROC curve (AUC = {roc_auc:.3f})')\n",
        "            ax2.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "            ax2.set_xlabel('False Positive Rate')\n",
        "            ax2.set_ylabel('True Positive Rate')\n",
        "            ax2.set_title('ROC Curve')\n",
        "            ax2.legend(loc=\"lower right\")\n",
        "            ax2.grid(True, alpha=0.3)\n",
        "\n",
        "            # 3. Risk vs Visits\n",
        "            ax3 = plt.subplot(2, 3, 3)\n",
        "            visit_counts = np.array(results['visit_count'])\n",
        "\n",
        "            scatter = ax3.scatter(visit_counts, risk_scores, alpha=0.6,\n",
        "                                 c=true_labels, cmap='coolwarm', s=50)\n",
        "            ax3.axhline(y=threshold, color='black', linestyle='--',\n",
        "                       label=f'Threshold', linewidth=2)\n",
        "\n",
        "            ax3.set_xlabel('Number of Visits')\n",
        "            ax3.set_ylabel('Predicted Risk')\n",
        "            ax3.set_title('Risk vs Visit Count')\n",
        "            ax3.legend()\n",
        "            ax3.grid(True, alpha=0.3)\n",
        "\n",
        "            plt.colorbar(scatter, ax=ax3, label='Outcome (0=Survived, 1=Died)')\n",
        "\n",
        "            # 4. Performance Metrics\n",
        "            ax4 = plt.subplot(2, 3, 4)\n",
        "            metrics = ['Sensitivity', 'Specificity', 'Precision', 'Recall', 'F1', 'Accuracy']\n",
        "\n",
        "            # Calculate metrics\n",
        "            pred_binary = np.array(results['predicted_label'])\n",
        "            accuracy = accuracy_score(true_labels, pred_binary)\n",
        "            precision = precision_score(true_labels, pred_binary, zero_division=0)\n",
        "            recall = recall_score(true_labels, pred_binary, zero_division=0)\n",
        "            f1 = f1_score(true_labels, pred_binary, zero_division=0)\n",
        "\n",
        "            cm = confusion_matrix(true_labels, pred_binary)\n",
        "            if cm.shape == (2, 2):\n",
        "                tn, fp, fn, tp = cm.ravel()\n",
        "                sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
        "                specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
        "            else:\n",
        "                sensitivity = 0\n",
        "                specificity = 0\n",
        "\n",
        "            values = [sensitivity, specificity, precision, recall, f1, accuracy]\n",
        "\n",
        "            bars = ax4.bar(metrics, values, alpha=0.7)\n",
        "            ax4.set_ylabel('Score')\n",
        "            ax4.set_title('Performance Metrics')\n",
        "            ax4.set_ylim([0, 1.1])\n",
        "\n",
        "            for bar, value in zip(bars, values):\n",
        "                height = bar.get_height()\n",
        "                ax4.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
        "                        f'{value:.3f}', ha='center', va='bottom')\n",
        "            ax4.grid(True, alpha=0.3)\n",
        "\n",
        "            # 5. High-Risk Patient Analysis\n",
        "            ax5 = plt.subplot(2, 3, 5)\n",
        "            high_risk_mask = risk_scores > threshold\n",
        "            high_risk_correct = true_labels[high_risk_mask] == 1\n",
        "            high_risk_incorrect = true_labels[high_risk_mask] == 0\n",
        "\n",
        "            categories = ['Correct\\nWarnings', 'False\\nWarnings', 'Missed\\nEvents', 'Correct\\nNegatives']\n",
        "            tp = np.sum(high_risk_correct)\n",
        "            fp = np.sum(high_risk_incorrect)\n",
        "            fn = np.sum((true_labels == 1) & (risk_scores <= threshold))\n",
        "            tn = np.sum((true_labels == 0) & (risk_scores <= threshold))\n",
        "\n",
        "            values = [tp, fp, fn, tn]\n",
        "            colors = ['green', 'orange', 'red', 'blue']\n",
        "\n",
        "            bars = ax5.bar(categories, values, color=colors, alpha=0.8)\n",
        "            ax5.set_ylabel('Number of Patients')\n",
        "            ax5.set_title('Patient Classification Results')\n",
        "\n",
        "            for bar, value in zip(bars, values):\n",
        "                height = bar.get_height()\n",
        "                ax5.text(bar.get_x() + bar.get_width()/2., height + 0.5,\n",
        "                        f'{value}', ha='center', va='bottom')\n",
        "            ax5.grid(True, alpha=0.3)\n",
        "\n",
        "            # 6. Risk Score Calibration\n",
        "            ax6 = plt.subplot(2, 3, 6)\n",
        "            # Simple calibration plot\n",
        "            bins = np.linspace(0, 1, 11)\n",
        "            bin_centers = (bins[:-1] + bins[1:]) / 2\n",
        "            predicted_rates = []\n",
        "            observed_rates = []\n",
        "\n",
        "            for i in range(len(bins)-1):\n",
        "                bin_mask = (risk_scores >= bins[i]) & (risk_scores < bins[i+1])\n",
        "                if np.sum(bin_mask) > 0:\n",
        "                    predicted_rate = np.mean(risk_scores[bin_mask])\n",
        "                    observed_rate = np.mean(true_labels[bin_mask])\n",
        "                    predicted_rates.append(predicted_rate)\n",
        "                    observed_rates.append(observed_rate)\n",
        "\n",
        "            if len(predicted_rates) > 0:\n",
        "                ax6.scatter(predicted_rates, observed_rates, s=100, alpha=0.7)\n",
        "                ax6.plot([0, 1], [0, 1], 'k--', alpha=0.5, label='Perfect Calibration')\n",
        "                ax6.set_xlabel('Predicted Risk')\n",
        "                ax6.set_ylabel('Observed Event Rate')\n",
        "                ax6.set_title('Risk Calibration')\n",
        "                ax6.legend()\n",
        "                ax6.grid(True, alpha=0.3)\n",
        "                ax6.set_xlim([0, 1])\n",
        "                ax6.set_ylim([0, 1])\n",
        "\n",
        "            plt.suptitle(f'Patient-Level Mortality Prediction (Threshold: {threshold:.3f})', fontsize=16)\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Visualization error: {e}\")\n",
        "\n",
        "def diagnose_data_structure(df):\n",
        "    \"\"\"Diagnose the structure of your data before processing\"\"\"\n",
        "    print(\"=\"*70)\n",
        "    print(\"DATA DIAGNOSIS\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    # Check key columns\n",
        "    required_cols = ['VISDAY', 'os_event', 'DSDAY', 'RPT']\n",
        "    for col in required_cols:\n",
        "        if col in df.columns:\n",
        "            print(f\"✓ {col}: Found ({df[col].notna().sum()} non-NA values)\")\n",
        "        else:\n",
        "            print(f\"✗ {col}: MISSING\")\n",
        "\n",
        "    # Check data quality\n",
        "    print(f\"\\nTotal records: {len(df)}\")\n",
        "    print(f\"Unique patients (RPT): {df['RPT'].nunique() if 'RPT' in df.columns else 'N/A'}\")\n",
        "\n",
        "    # Check os_event distribution\n",
        "    print(f\"\\nos_event distribution:\")\n",
        "    print(f\"  os_event = 0: {df[df['os_event'] == 0].shape[0]} records\")\n",
        "    print(f\"  os_event = 1: {df[df['os_event'] == 1].shape[0]} records\")\n",
        "    print(f\"  Missing os_event: {df['os_event'].isna().sum()} records\")\n",
        "\n",
        "    # Check patients who die\n",
        "    death_patients = df[df['os_event'] == 1]\n",
        "    if len(death_patients) > 0:\n",
        "        print(f\"\\nPatients who die (os_event=1):\")\n",
        "        print(f\"  Count: {death_patients['RPT'].nunique() if 'RPT' in df.columns else 'N/A'}\")\n",
        "\n",
        "        # Sample a patient who dies\n",
        "        sample_patient = death_patients['RPT'].iloc[0] if 'RPT' in death_patients.columns else None\n",
        "        if sample_patient:\n",
        "            patient_data = df[df['RPT'] == sample_patient]\n",
        "            print(f\"\\nSample patient {sample_patient} (who dies):\")\n",
        "            print(patient_data[['VISDAY', 'DSDAY', 'os_event']].sort_values('VISDAY').head(10))\n",
        "\n",
        "            # Check consistency\n",
        "            unique_dsdays = patient_data['DSDAY'].unique()\n",
        "            non_na_dsdays = unique_dsdays[~pd.isna(unique_dsdays)]\n",
        "            print(f\"  Unique DSDAY values: {len(unique_dsdays)} (non-NA: {len(non_na_dsdays)})\")\n",
        "            if len(non_na_dsdays) == 1:\n",
        "                print(f\"  Death day: {non_na_dsdays[0]}\")\n",
        "\n",
        "    # Check VISDAY vs DSDAY relationship\n",
        "    print(f\"\\nVISDAY vs DSDAY analysis:\")\n",
        "    if 'VISDAY' in df.columns and 'DSDAY' in df.columns:\n",
        "        # Check for visits after death\n",
        "        visits_after_death = df[df['VISDAY'] > df['DSDAY']]\n",
        "        print(f\"  Visits after death (VISDAY > DSDAY): {len(visits_after_death)}\")\n",
        "\n",
        "        # Check for visits on death day\n",
        "        visits_on_death = df[df['VISDAY'] == df['DSDAY']]\n",
        "        print(f\"  Visits on death day (VISDAY == DSDAY): {len(visits_on_death)}\")\n",
        "\n",
        "        # Check for visits before death\n",
        "        visits_before_death = df[df['VISDAY'] < df['DSDAY']]\n",
        "        print(f\"  Visits before death (VISDAY < DSDAY): {len(visits_before_death)}\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "\n",
        "# Use it in your main function:\n",
        "def run_clinical_early_warning():\n",
        "    \"\"\"Main function to run clinical early warning pipeline\"\"\"\n",
        "    print(\"CLINICAL EARLY WARNING SYSTEM FOR MORTALITY PREDICTION\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    try:\n",
        "        # Load data\n",
        "        print(\"\\n1. Loading data...\")\n",
        "        pcdata = pd.read_csv('pcdata.csv')\n",
        "        print(f\"   Loaded {len(pcdata)} samples with {len(pcdata.columns)} features\")\n",
        "\n",
        "        # Run diagnosis\n",
        "        diagnose_data_structure(pcdata)\n",
        "\n",
        "        # Check for required columns\n",
        "        required_cols = ['VISDAY', 'os_event', 'DSDAY']\n",
        "        missing_cols = [col for col in required_cols if col not in pcdata.columns]\n",
        "        if missing_cols:\n",
        "            print(f\"Error: Missing required columns: {missing_cols}\")\n",
        "            return None, None\n",
        "\n",
        "        # Initialize and run pipeline\n",
        "        warning_pipeline = ClinicalEarlyWarningPipeline(\n",
        "            prediction_horizon_days=90,  # 90-day prediction horizon\n",
        "            n_splits=3,\n",
        "            random_state=42,\n",
        "            use_autoencoder=True\n",
        "        )\n",
        "\n",
        "        # Run analysis\n",
        "        #results = warning_pipeline.run_pipeline(pcdata)\n",
        "        # Run patient-level analysis\n",
        "        results = warning_pipeline.run_patient_level_pipeline(pcdata)\n",
        "\n",
        "        if results:\n",
        "            # Save patient-level predictions\n",
        "            df_predictions = pd.DataFrame({\n",
        "                'patient_id': results['patient_id'],\n",
        "                'predicted_risk': results['predicted_risk'],\n",
        "                'predicted_label': results['predicted_label'],\n",
        "                'true_label': results['true_label'],\n",
        "                'days_to_event': results['days_to_event'],\n",
        "                'visit_count': results['visit_count']\n",
        "            })\n",
        "\n",
        "            # Add high-risk flag\n",
        "            threshold = results['threshold_used']\n",
        "            df_predictions['high_risk'] = df_predictions['predicted_risk'] > threshold\n",
        "\n",
        "            print(f\"\\nSAVED {len(df_predictions)} PATIENT PREDICTIONS\")\n",
        "            print(f\"High-risk patients flagged: {df_predictions['high_risk'].sum()}\")\n",
        "\n",
        "            # Save to CSV\n",
        "            df_predictions.to_csv('patient_mortality_predictions.csv', index=False)\n",
        "            print(\"Predictions saved to 'patient_mortality_predictions.csv'\")\n",
        "\n",
        "            return warning_pipeline, results\n",
        "\n",
        "        return None, None\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        return None, None\n",
        "\n",
        "\n",
        "# Run the patient-level pipeline\n",
        "if __name__ == \"__main__\":\n",
        "    warning_pipeline, warning_results = run_clinical_early_warning()"
      ]
    }
  ]
}